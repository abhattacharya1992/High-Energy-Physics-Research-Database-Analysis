# -*- coding: utf-8 -*-
"""HEP Research Database Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ey6vydQjIYS7h-sXIIKoSaIy_XjYAGN7

# Topic Evolution in High-Energy Physics
*Exploring scientific topic dynamics using BERTopic and time-series forecasting*

## Abstract
This notebook builds a citation-aware corpus of particle-physics publications by extracting arXiv/DOI identifiers from seed PDFs and expanding via arXiv, Crossref, Semantic Scholar, and INSPIRE. I apply BERTopic to discover high-level research themes, analyze their temporal evolution (normalized yearly shares, 3-year smoothing, 5-year bins), decompose trends (STL), and forecast near-term dynamics with ARIMA. I complement this with supervised explainability (XGBoost feature importance) to identify domain terms that drive topic distinctions. Results capture the rise of collider/Higgs-centric topics and a recent stabilization phase.

## Motivation
- **Map the field**: Understand how collider/QCD/Higgs themes evolved across decades and LHC eras.  
- **Quantify change**: Identify emerging/declining topics using simple, reproducible time-series tools.  
- **Interpret models**: Use feature importances to tie language signals to domain concepts.  
- **Reusable pipeline**: Provide an end-to-end, citation-aware workflow from raw PDFs to insights.

## Environment Setup and Package Installation

This section ensures all required Python libraries are installed and ready for use in Colab.
"""

!pip -q install sentence-transformers
from sentence_transformers import SentenceTransformer

!pip install yake

pip install gensim

# PDF + data wrangling
!pip install -q pymupdf pandas tqdm requests

# NLP pre-processing
!pip install -q spacy==3.7.4 scikit-learn
!python -m spacy download en_core_web_sm

# Topic modeling
!pip install -q bertopic umap-learn hdbscan numpy

import pandas as pd
import urllib.request, os
import fitz
import re, time, html, requests
from collections import defaultdict
from tqdm import tqdm
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np, pandas as pd
from bertopic import BERTopic
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from pathlib import Path

"""## Defining generic functions for later use"""

doi_core_re     = re.compile(r"10\.\s*\d{4,9}\s*/\s*[A-Z0-9._;()/:\-]+", re.I)
doi_url_re      = re.compile(r"https?://(?:dx\.)?doi\.org/([A-Z0-9./;()_\-]+)", re.I)
modern_arxiv_re = re.compile(r"(?:arxiv\s*:\s*)?(\d{4}\s*\.\s*\d{4,5})(?:\s*v\d+)?", re.I)
legacy_arxiv_re = re.compile(r"(?:arxiv\s*:\s*)?([a-z\-]+(?:\.[A-Za-z]{2})?\s*/\s*\d{7})(?:\s*v\d+)?", re.I)
arxiv_url_re    = re.compile(r"https?://(?:www\.)?arxiv\.org/abs/([A-Za-z0-9.\-]+/[0-9]{7}|[0-9]{4}\.[0-9]{4,5}(?:v[0-9]+)?)", re.I)

_modern_arxiv = re.compile(r"^(\d{4})\.(\d{4,5})$")                 # modern: yymm.number
_legacy_arxiv = re.compile(r"^[a-z\-]+(\.[a-z]{2})?/\d{7}$", re.I)  # legacy: hep-ph/xxxxxxx

ARXIV_API = "http://export.arxiv.org/api/query"
SS_BASE   = "https://api.semanticscholar.org/graph/v1/paper/"
CR_BASE   = "https://api.crossref.org/works"
HEADERS   = {"User-Agent": "nlp-thesis-bot/1.0 (mailto:your@email.com)"}

# =========================
# 3) HELPERS (same logic as your notebook)  ─── keep consistent
# =========================
def clean_token(tok: str) -> str:
    return (tok or "").replace("\u00ad","").replace("\ufeff","").replace("\u200b","").strip()

def normalize_page_text(s: str) -> str:
    if not s: return ""
    s = s.replace("\u00ad","").replace("\ufeff","").replace("\u200b","")
    s = s.replace("\r","\n")
    s = re.sub(r"(\w)-\s*\n\s*(\w)", r"\1\2", s)      # de-hyphenate
    s = re.sub(r"\s+", " ", s)                        # collapse ws
    return s

def norm_doi(s: str) -> str:
    s = (s or "").strip()
    s = s.replace("DX.DOI.ORG/","doi.org/").replace("dx.doi.org/","doi.org/")
    s = s.replace("DOI:","doi:").replace("doi:","").replace("doi.org/","")
    s = s.lower().replace(" ","").rstrip('.,);]')
    return s

def norm_arxiv(s: str) -> str:
    s = (s or "").strip().replace("arXiv:","").replace("ARXIV:","").replace(" ","")
    s = re.sub(r'v\d+$', "", s)
    return s.lower()

DOI_BODY_CHARS = set("abcdefghijklmnopqrstuvwxyz0123456789._;()/:-")
def looks_like_doi_start(tok: str) -> bool: return tok.lower().startswith("10.")
def looks_like_doi_part(tok: str) -> bool:
    t = tok.lower().rstrip('.,);]')
    return t != "" and all(c in DOI_BODY_CHARS for c in t)

def looks_like_arxiv_modern(tok: str) -> bool:
    return bool(re.fullmatch(r"\d{4}\.?\d{4,5}", tok))
def looks_like_arxiv_legacy(tok: str) -> bool:
    return bool(re.fullmatch(r"[a-z\-]+(\.[A-Za-z]{2})?/\d{7}", tok, re.I))

def extract_ids_from_pdf(path: str, label: str) -> pd.DataFrame:
    doc = fitz.open(path)
    n_pages = len(doc)
    recs = []

    # A) token stream sweep
    for p in range(n_pages):
        words = doc[p].get_text("words")
        words.sort(key=lambda w: (round(w[1],1), round(w[0],1)))
        i, W = 0, len(words)
        while i < W:
            tok = clean_token(words[i][4]); low = tok.lower()

            # DOI
            if looks_like_doi_start(tok) or low in {"doi","doi:","dx.doi.org","doi.org"}:
                j = i
                if not looks_like_doi_start(tok):
                    # find a nearby "10."
                    for k in range(i+1, min(i+10, W)):
                        if looks_like_doi_start(clean_token(words[k][4])):
                            j = k; break
                    else:
                        i += 1; continue
                doi_tokens = [clean_token(words[j][4])]
                k = j + 1
                while k < W:
                    nxt = clean_token(words[k][4])
                    if looks_like_doi_part(nxt):
                        doi_tokens.append(nxt); k += 1
                    else:
                        trimmed = nxt.rstrip('.,);]')
                        if trimmed and looks_like_doi_part(trimmed):
                            doi_tokens.append(trimmed); k += 1
                        else:
                            break
                doi_norm = norm_doi("".join(doi_tokens))
                if "/" in doi_norm and re.search(r"[a-z]", doi_norm.split("/",1)[1] or ""):
                    recs.append({"source_pdf": label, "page": p+1, "id_type": "doi",
                                 "raw": " ".join(doi_tokens), "normalized": doi_norm, "where": "tokens"})
                i = k; continue

            # modern arXiv possibly split
            if re.fullmatch(r"\d{4}\.?", tok):
                if i+1 < W:
                    nxt = clean_token(words[i+1][4])
                    cand = (tok + nxt).replace(" ","")
                    if looks_like_arxiv_modern(cand.replace(".","")):
                        aid = cand if "." in cand else tok + "." + nxt
                        recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                                     "raw": f"{tok} {nxt}", "normalized": norm_arxiv(aid), "where": "tokens"})
                        i += 2; continue

            if looks_like_arxiv_modern(tok):
                recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                             "raw": tok, "normalized": norm_arxiv(tok), "where": "tokens"})
                i += 1; continue

            # legacy arXiv split
            if re.fullmatch(r"[a-z\-]+(\.[A-Za-z]{2})?", tok, re.I) and i+2 < W and words[i+1][4] == "/":
                nxt = clean_token(words[i+2][4])
                if re.fullmatch(r"\d{7}", nxt):
                    aid = f"{tok}/{nxt}"
                    recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                                 "raw": aid, "normalized": norm_arxiv(aid), "where": "tokens"})
                    i += 3; continue

            if looks_like_arxiv_legacy(tok):
                recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                             "raw": tok, "normalized": norm_arxiv(tok), "where": "tokens"})
                i += 1; continue

            # arXiv URL inside token
            if "arxiv.org/abs/" in low:
                m = re.search(r'arxiv\.org/abs/([A-Za-z0-9.\-]+/[0-9]{7}|[0-9]{4}\.[0-9]{4,5}(?:v[0-9]+)?)', low)
                if m:
                    recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                                 "raw": m.group(0), "normalized": norm_arxiv(m.group(1)), "where": "tokens"})
                i += 1; continue

            i += 1

    # B) normalized page text (regex)
    for p in range(n_pages):
        txt = normalize_page_text(doc[p].get_text("text") or "")
        for m in doi_core_re.finditer(txt):
            recs.append({"source_pdf": label, "page": p+1, "id_type": "doi",
                         "raw": m.group(0), "normalized": norm_doi(m.group(0)), "where": "text"})
        for m in doi_url_re.finditer(txt):
            recs.append({"source_pdf": label, "page": p+1, "id_type": "doi",
                         "raw": m.group(0), "normalized": norm_doi(m.group(1)), "where": "text"})
        for m in arxiv_url_re.finditer(txt):
            recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                         "raw": m.group(0), "normalized": norm_arxiv(m.group(1)), "where": "text"})
        for m in modern_arxiv_re.finditer(txt):
            recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                         "raw": m.group(0), "normalized": norm_arxiv(m.group(1)), "where": "text"})
        for m in legacy_arxiv_re.finditer(txt):
            recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                         "raw": m.group(0), "normalized": norm_arxiv(m.group(0)), "where": "text"})

    # C) link annotations
    for p in range(n_pages):
        try:
            for ln in doc[p].get_links():
                uri = ln.get("uri")
                if not uri: continue
                L = uri.lower()
                if "doi.org/" in L or "dx.doi.org/" in L:
                    m = re.search(r"(?:dx\.)?doi\.org/([A-Za-z0-9./;()_\-]+)", L)
                    if m:
                        recs.append({"source_pdf": label, "page": p+1, "id_type": "doi",
                                     "raw": uri, "normalized": norm_doi(m.group(1)), "where": "link"})
                if "arxiv.org/abs/" in L:
                    m = re.search(r"arxiv\.org/abs/([A-Za-z0-9.\-]+/[0-9]{7}|[0-9]{4}\.[0-9]{4,5}(?:v[0-9]+)?)", L)
                    if m:
                        recs.append({"source_pdf": label, "page": p+1, "id_type": "arxiv_id",
                                     "raw": uri, "normalized": norm_arxiv(m.group(1)), "where": "link"})
        except Exception:
            pass

    return pd.DataFrame.from_records(recs)

# =========================
# 1) RATE LIMIT SAFETY
# =========================
def safe_sleep(seconds: float = 0.6):
    """Pause between API calls to avoid rate limiting."""
    time.sleep(seconds)

# =========================
# 2) ARXIV + DOI CLEANERS
# =========================
def clean_arxiv(raw: str) -> str:
    """Normalize arXiv IDs and strip version suffixes."""
    if not isinstance(raw, str):
        return ""
    s = raw.strip().replace("arXiv:", "").replace("ARXIV:", "")
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"v\d+$", "", s)
    s = re.sub(r"[^a-zA-Z0-9./\-]", "", s)
    return s.lower()

def is_valid_arxiv_id(aid: str) -> bool:
    """Check modern (yymm.number) or legacy ID is plausible."""
    m = _modern_arxiv.fullmatch(aid)
    if m:
        yy, mm = int(m.group(1)[:2]), int(m.group(1)[2:])
        if not (1 <= mm <= 12):  # real month
            return False
        return (yy * 100 + mm) >= 704  # arXiv modern IDs start ~0704
    return _legacy_arxiv.fullmatch(aid) is not None

def filter_valid_arxiv_ids(ids):
    """Clean and filter a list of raw arXiv IDs."""
    good, bad = [], []
    for x in ids:
        a = clean_arxiv(x)
        if is_valid_arxiv_id(a):
            good.append(a)
        else:
            bad.append(x)
    if bad:
        print(f"Filtered {len(bad)} invalid arXiv matches (e.g., {bad[:5]})")
    return sorted(set(good))

def clean_doi(raw: str) -> str:
    """Normalize DOIs (strip prefixes and spaces)."""
    if not isinstance(raw, str):
        return ""
    s = raw.strip().lower()
    s = re.sub(r"https?://(dx\.)?doi\.org/", "", s)
    s = re.sub(r"^doi:?", "", s)
    s = re.sub(r"\s+", "", s)
    return s if s.startswith("10.") else ""

# =========================
# 3) PDF DOWNLOAD VALIDATION
# =========================
def arxiv_pdf_url(aid: str) -> str:
    """Return canonical arXiv PDF URL."""
    return f"https://arxiv.org/pdf/{aid}.pdf"

def is_downloadable_arxiv(aid: str, timeout=12) -> bool:
    """Check if a given arXiv ID has a valid, downloadable PDF."""
    try:
        url = arxiv_pdf_url(aid)
        r = requests.head(url, allow_redirects=True, timeout=timeout)
        if r.status_code == 405:  # HEAD not allowed
            r = requests.get(url, stream=True, timeout=timeout)
        return r.status_code == 200
    except Exception:
        return False

# =========================
# 4) METADATA FETCHERS
# =========================
def fetch_arxiv(aid: str):
    """Fetch metadata directly from the arXiv OAI API."""
    try:
        r = requests.get(ARXIV_API, params={"id_list": aid}, headers=HEADERS, timeout=25)
        if r.status_code != 200:
            return None
        t = re.search(r"<title>(.*?)</title>", r.text, re.DOTALL)
        s = re.search(r"<summary>(.*?)</summary>", r.text, re.DOTALL)
        y = re.search(r"<published>(\d{4})", r.text)
        a = re.findall(r"<name>(.*?)</name>", r.text)
        u = re.search(r'<link rel="alternate" type="text/html" href="([^"]+)"', r.text)
        return {
            "title": html.unescape(t.group(1).strip()) if t else None,
            "abstract": html.unescape(s.group(1).strip()) if s else None,
            "year": int(y.group(1)) if y else None,
            "authors": ", ".join([html.unescape(x.strip()) for x in a]) if a else None,
            "url": u.group(1) if u else f"https://arxiv.org/abs/{aid}",
            "source": "arxiv"
        }
    except Exception:
        return None

def fetch_semantic_scholar(key: str):
    """Fallback: fetch metadata from Semantic Scholar API (works for DOI or arXiv)."""
    if key.startswith("10."):
        key = f"DOI:{key}"
    elif re.fullmatch(r"\d{4}\.\d{4,5}", key):
        key = f"arXiv:{key}"
    try:
        r = requests.get(
            SS_BASE + key,
            params={"fields": "title,abstract,year,authors,url"},
            headers=HEADERS,
            timeout=25,
        )
        if r.status_code != 200:
            return None
        j = r.json()
        return {
            "title": j.get("title"),
            "abstract": j.get("abstract"),
            "year": j.get("year"),
            "authors": ", ".join([a.get("name") for a in j.get("authors", [])]),
            "url": j.get("url"),
            "source": "semanticscholar"
        }
    except Exception:
        return None

def fetch_crossref(doi: str):
    """Fetch metadata from Crossref (journals)."""
    try:
        r = requests.get(f"{CR_BASE}/{doi}", headers=HEADERS, timeout=25)
        if r.status_code != 200:
            return None
        m = r.json().get("message", {})
        year = None
        for k in ("published-print", "published-online", "issued"):
            if m.get(k, {}).get("date-parts"):
                year = m[k]["date-parts"][0][0]
                break
        return {
            "title": (m.get("title") or [None])[0],
            "abstract": re.sub("<[^>]+>", "", m.get("abstract") or "").strip()
                        if m.get("abstract") else None,
            "year": year,
            "authors": ", ".join([
                f"{a.get('given','')} {a.get('family','')}".strip()
                for a in m.get("author", [])
            ]) if m.get("author") else None,
            "url": m.get("URL"),
            "source": "crossref"
        }
    except Exception:
        return None

# =========================
# Multi-hop arXiv expansion
# =========================
def crawl_arxiv_refs(
    seed_arxiv_ids,
    max_depth=2,
    max_papers=300,           # cap total downloaded PDFs
    sleep_between=0.6,        # seconds between API calls
    out_dir="crawl_outputs"   # where to drop CSVs and PDFs
):
    Path(out_dir).mkdir(parents=True, exist_ok=True)

    # State
    downloaded = set()      # arXiv IDs whose PDFs we downloaded
    seen_ids   = set()      # all normalized IDs (arXiv+DOI) we have seen
    # Start with seeds (already validated earlier ideally)
    frontier = list(filter_valid_arxiv_ids(seed_arxiv_ids))

    # Collected metadata rows across depths
    all_meta_rows = []
    all_id_frames = []

    # Helper: safe csv save
    def _save(df, name):
        p = os.path.join(out_dir, name)
        df.to_csv(p, index=False)
        print(f"Saved → {p}")

    # Depth loop
    total_downloads = 0
    for depth in range(1, max_depth + 1):
        print(f"\n=== Depth {depth} ===")

        # Validate & preflight this depth's frontier
        frontier = filter_valid_arxiv_ids(frontier)
        frontier = [a for a in frontier if a not in downloaded and is_downloadable_arxiv(a)]
        print(f"Frontier size (ready to download PDFs): {len(frontier)}")

        if not frontier:
            print("No more valid/downloadable arXiv IDs. Stopping.")
            break

        # Hard cap
        if total_downloads >= max_papers:
            print(f"Reached max_papers={max_papers}. Stopping.")
            break

        # Collect references found at this depth
        depth_refs_frames = []
        next_frontier = []

        for aid in frontier:
            if total_downloads >= max_papers:
                print(f"Reached max_papers={max_papers}. Stopping this depth.")
                break

            try:
                pdf_url  = arxiv_pdf_url(aid)
                pdf_path = os.path.join(out_dir, f"{aid.replace('/', '_')}.pdf")
                if not os.path.exists(pdf_path):
                    urllib.request.urlretrieve(pdf_url, pdf_path)
                downloaded.add(aid)
                total_downloads += 1

                # Extract ids (this captures references)
                df_refs = extract_ids_from_pdf(pdf_path, label=f"depth{depth}_{aid}")
                if not df_refs.empty:
                    depth_refs_frames.append(df_refs)

                # Fetch metadata for the current paper itself (depth node)
                meta = fetch_arxiv(aid) or fetch_semantic_scholar(aid) or {}
                safe_sleep(sleep_between)
                all_meta_rows.append({
                    "paper_key": aid, "key_type": "arxiv",
                    "title": meta.get("title"), "abstract": meta.get("abstract"),
                    "year": meta.get("year"), "authors": meta.get("authors"),
                    "url": meta.get("url") or f"https://arxiv.org/abs/{aid}",
                    "source": meta.get("source") or "arxiv_or_ss", "depth": depth
                })

            except Exception as e:
                print(f"Skip {aid}: {e}")
                continue

        # Combine all refs found at this depth
        depth_refs = pd.concat(depth_refs_frames, ignore_index=True) if depth_refs_frames else pd.DataFrame(
            columns=["source_pdf","page","id_type","raw","normalized","where"]
        )
        # Dedupe and save
        if not depth_refs.empty:
            depth_refs = depth_refs.dropna(subset=["normalized"]).drop_duplicates(subset=["id_type","normalized"])
        all_id_frames.append(depth_refs)
        _save(depth_refs, f"refs_depth{depth}.csv")

        # Add to seen set
        if not depth_refs.empty:
            for x in depth_refs["normalized"]:
                seen_ids.add(x)

        # Build next frontier only from arXiv IDs in refs
        if not depth_refs.empty:
            new_arxiv = depth_refs.query("id_type=='arxiv_id'")["normalized"].tolist()
            # Clean & filter; also exclude what we already downloaded/queued
            next_frontier = [a for a in filter_valid_arxiv_ids(new_arxiv)
                             if a not in downloaded]

        # Move frontier
        frontier = next_frontier

    # After expansion: also fetch metadata for DOIs we saw along the way (no crawling via DOI)
    if all_id_frames:
        all_ids_union = pd.concat(all_id_frames, ignore_index=True)
    else:
        all_ids_union = pd.DataFrame(columns=["id_type","normalized"])
    doi_ids = sorted(set(
        clean_doi(x) for x in all_ids_union.query("id_type=='doi'")["normalized"].dropna().tolist()
        if clean_doi(x)
    ))
    print(f"Collecting metadata for {len(doi_ids)} DOIs found during crawl...")
    for doi in doi_ids:
        meta = fetch_semantic_scholar(doi) or fetch_crossref(doi) or {}
        safe_sleep(sleep_between)
        all_meta_rows.append({
            "paper_key": doi, "key_type": "doi",
            "title": meta.get("title"), "abstract": meta.get("abstract"),
            "year": meta.get("year"), "authors": meta.get("authors"),
            "url": meta.get("url"), "source": meta.get("source") or "ss_or_crossref",
            "depth": None
        })

    # Final corpus
    corpus = pd.DataFrame(all_meta_rows).drop_duplicates(subset=["paper_key"])
    _save(corpus, "corpus_expanded.csv")

    # Also return for immediate use
    return corpus, all_ids_union

"""## Parsing PDFs and Extracting References

Extract arXiv IDs and DOIs from the bibliography sections of my personal and seed publications.
"""

# =========================
# 1) PDF LIST (edit here)
# =========================
PDFS = [
    ("https://arxiv.org/pdf/2403.07615", "main.pdf",          "thesis_main"),
    ("https://arxiv.org/pdf/2112.02341", "2112.02341.pdf",    "paper_psA_NNLO+NNLL"),
    ("https://arxiv.org/pdf/1909.08993", "1909.08993.pdf",    "dipseudo_2loop"),
    ("https://arxiv.org/pdf/2502.20331", "2502.20331.pdf",    "VH_resum")
]

# =========================
# 2) DOWNLOAD ALL
# =========================
for url, path, _ in PDFS:
    if not os.path.exists(path):
        urllib.request.urlretrieve(url, path)
        print("Downloaded:", os.path.abspath(path))

# print first N words of the pdfs
for url, PDF_PATH, label in PDFS: # Iterate through the tuples in PDFS
    doc = fitz.open(PDF_PATH) # Use the second element of the tuple (the file path)
    print(f"File: {PDF_PATH}") # Print the file path
    print("Pages:", len(doc))
    nwords = 1000 # Define nwords once
    print(f"First {nwords} chars of page 1:")
    print(doc[0].get_text("text")[:nwords])
    print("\n")
    print("-" * 20) # Add a separator for clarity

"""### I am now extracting the arXiv/DOIs of the references in the bibliography of the four publications/thesis imported before."""

# =========================
# EXTRACT FROM ALL PDFs → CONCAT → DEDUPE
# =========================
dfs = []
for _, path, label in PDFS:
    df_part = extract_ids_from_pdf(path, label)
    dfs.append(df_part)

ids_all = pd.concat(dfs, ignore_index=True)
# treat legacy uniformly (already normalized), then dedupe
ids_all = ids_all.sort_values(["id_type","normalized","source_pdf","page","where"]).drop_duplicates(
    subset=["id_type","normalized"], keep="first"
).reset_index(drop=True)

print(f"TOTAL unique identifiers across PDFs: {len(ids_all)}")
print(ids_all["id_type"].value_counts(dropna=False))
ids_all.to_csv("all_identifiers.csv", index=False)
print("Saved → all_identifiers.csv")

# =========================
# 4) FETCH METADATA FOR THOSE IDs (arXiv/Crossref/Semantic Scholar)
# =========================
arxiv_ids = sorted({clean_arxiv(x) for x in ids_all.query("id_type=='arxiv_id'")["normalized"] if pd.notna(x)})
doi_ids   = sorted({clean_doi(x)   for x in ids_all.query("id_type=='doi'")["normalized"]      if pd.notna(x)})

rows = []
for i, aid in enumerate(arxiv_ids, 1):
    if not is_downloadable_arxiv(aid):
        continue
    meta = fetch_arxiv(aid) or fetch_semantic_scholar(aid)
    safe_sleep()
    rows.append({"paper_key": aid, "key_type": "arxiv", **(meta or {})})
    if i % 10 == 0:
        print(f"Processed {i}/{len(arxiv_ids)} arXiv IDs…")

for doi in doi_ids:
    meta = fetch_semantic_scholar(doi) or fetch_crossref(doi)
    safe_sleep()
    rows.append({"paper_key": doi, "key_type": "doi", **(meta or {})})

corpus = pd.DataFrame(rows).drop_duplicates(subset="paper_key")
corpus.to_csv("citations_corpus_all.csv", index=False)
print(f"Saved → citations_corpus_all.csv ({len(corpus)} rows)")

"""### Removing invalid entries"""

arxiv_ids = filter_valid_arxiv_ids(
    ids_all.query("id_type=='arxiv_id'")["normalized"].dropna().tolist()
)

"""### After extracting the seed references from those 4 publications, I move on to explore the references listed within them correctly identified within my publications"""

expanded_corpus, all_depth_ids = crawl_arxiv_refs(
    seed_arxiv_ids=arxiv_ids,
    max_depth=2,        # increase to go deeper
    max_papers=150,     # safety limit
    out_dir="crawl_outputs"
)

# Load (in case you want to restart from disk)
expanded_corpus = pd.read_csv("crawl_outputs/corpus_expanded.csv")

print(f"Shape of basic corpus   =  {corpus.shape}")
print(f"Shape of updated corpus =  {expanded_corpus.shape}")

"""### Dropping NaN entries"""

expanded_corpus = expanded_corpus.dropna(subset=["abstract", "year"])
expanded_corpus["clean_abstract"] = (
    expanded_corpus["abstract"]
    .astype(str)
    .str.replace(r"\s+", " ", regex=True)
    .str.replace(r"http\S+", "", regex=True)
    .str.strip()
)
expanded_corpus["year"] = pd.to_numeric(expanded_corpus["year"], errors="coerce").astype("Int64")
expanded_corpus = expanded_corpus.dropna(subset=["year"])

print(f"Shape of updated courpus after removing NaN values =  {expanded_corpus.shape}")

display(expanded_corpus)

"""## Expanding the Citation Network

Use extracted IDs to crawl citations up to depth 3 using the arXiv, CrossRef, Semantic Scholar, and INSPIRE APIs. This builds a citation graph rooted in my work.
"""

# --- Expand via INSPIRE citations (compatible with your expanded_corpus schema) ---
SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "hep-topic-evolution <winx_0492@yahoo.com>"})

def _get_json(url, params=None, sleep=0.25):
    r = SESSION.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    time.sleep(sleep)
    return data

# --- arXiv normalization: handle v2, "arXiv:" prefix, and old IDs like hep-ph/9700001
NEW_ARXIV = re.compile(r"(?i)(?:arxiv:)?(\d{4}\.\d{4,5})(?:v\d+)?$")
OLD_ARXIV = re.compile(r"(?i)(?:arxiv:)?([a-z\-]+\/\d{7})(?:v\d+)?$")

def normalize_arxiv_id(x: str) -> str | None:
    if not isinstance(x, str): return None
    x = x.strip()
    m = NEW_ARXIV.fullmatch(x) or OLD_ARXIV.fullmatch(x)
    return m.group(1) if m else None

def get_inspire_recid_from_arxiv(arxiv_id: str) -> int | None:
    """Try a couple of query variants to be robust."""
    base = "https://inspirehep.net/api/literature"
    # 1) canonical arxiv: query
    for q in (f"arxiv:{arxiv_id}", f"eprint:{arxiv_id}"):
        data = _get_json(base, params={"q": q, "size": 1})
        hits = data.get("hits", {}).get("hits", [])
        if hits:
            return hits[0]["metadata"]["control_number"]
    return None

def iterate_citing_hits(recid: int, cap: int = 200):
    """Yield citing records for a recid, following pagination via links.next."""
    url = "https://inspirehep.net/api/literature"
    params = {"q": f"refersto:recid:{recid}", "size": 50}
    fetched = 0
    while url and fetched < cap:
        data = _get_json(url, params=params)
        hits = data.get("hits", {}).get("hits", [])
        for h in hits:
            yield h
            fetched += 1
            if fetched >= cap:
                break
        # follow pagination
        link_next = (data.get("links") or {}).get("next")
        url, params = (link_next, None) if link_next else (None, None)

def hit_to_row(hit: dict) -> dict:
    md = hit.get("metadata", {}) or {}
    title = (md.get("titles") or [{}])[0].get("title")
    abstract = (md.get("abstracts") or [{}])[0].get("value")
    earliest = md.get("earliest_date")
    year = int(str(earliest)[:4]) if earliest else None
    # arXiv / DOI extraction
    arxiv_key = None
    for ext in (md.get("external_system_identifiers") or []):
        if ext.get("schema") == "arXiv":
            arxiv_key = normalize_arxiv_id(ext.get("value", ""))
            break
    dois = [d.get("value") for d in (md.get("dois") or [])]
    doi_val = dois[0].lower() if dois else None
    authors = ", ".join(a.get("full_name","") for a in (md.get("authors") or []))
    # Prefer arXiv link; fallback to DOI resolver; else None
    url = f"https://arxiv.org/abs/{arxiv_key}" if arxiv_key else (f"https://doi.org/{doi_val}" if doi_val else None)

    # Map to your schema
    if arxiv_key:
        paper_key, key_type = arxiv_key, "arxiv"
    elif doi_val:
        paper_key, key_type = doi_val, "doi"
    else:
        paper_key, key_type = title or "", "unknown"

    return {
        "paper_key": paper_key,
        "key_type": key_type,
        "title": title,
        "abstract": abstract,
        "year": year,
        "authors": authors,
        "url": url,
        "source": "inspire_citation",
        "depth": 3.0,  # "depth" is your own field; use 3 for "citers of depth-2"
    }

def expand_via_inspire_citations(expanded_corpus: pd.DataFrame,
                                 per_paper_cap: int = 120,
                                 arxiv_col: str = "paper_key",
                                 keytype_col: str = "key_type") -> pd.DataFrame:
    seeds = expanded_corpus.loc[expanded_corpus[keytype_col].str.lower().eq("arxiv"), [arxiv_col]]
    seeds["arxiv_norm"] = seeds[arxiv_col].map(normalize_arxiv_id)
    seeds = seeds.dropna(subset=["arxiv_norm"]).drop_duplicates("arxiv_norm")

    print(f"Seed arXiv papers to resolve: {len(seeds)}.\n")
    rows, seen = [], set()
    resolved_ok = 0

    for aid in seeds["arxiv_norm"]:
        recid = get_inspire_recid_from_arxiv(aid)
        if not recid:
            continue
        resolved_ok += 1

        # Add this here:
        if resolved_ok % 20 == 0:
            print(f"{resolved_ok} seeds resolved so far...")

        for hit in iterate_citing_hits(recid, cap=per_paper_cap):
            row = hit_to_row(hit)
            key = (row["key_type"], row["paper_key"])
            if not row["paper_key"]:
                key = ("title", row.get("title") or "")
            if key not in seen:
                seen.add(key)
                rows.append(row)

    print(f"Resolved recids: {resolved_ok} / {len(seeds)}")
    print(f"Raw citing records collected: {len(rows)}")

    return pd.DataFrame(rows)

# Optional: create clean_abstract for new rows
def _clean(s: str) -> str:
    import re
    s = str(s or "")
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"http\S+", "", s)
    return s.strip()

# ---- Run expansion
citers = expand_via_inspire_citations(expanded_corpus, per_paper_cap=60)

print("Citing rows:", len(citers))

# Deduplicate against existing expanded_corpus on (key_type, paper_key)
existing_pairs = set(zip(expanded_corpus["key_type"].fillna(""), expanded_corpus["paper_key"].fillna("")))
mask_new = ~citers.apply(lambda r: (r.get("key_type",""), r.get("paper_key","")) in existing_pairs, axis=1)
citers = citers[mask_new].reset_index(drop=True)

# Merge
augmented_corpus = pd.concat([expanded_corpus, citers], ignore_index=True)

augmented_corpus.loc[augmented_corpus["clean_abstract"].isna(), "clean_abstract"] = \
    augmented_corpus.loc[augmented_corpus["clean_abstract"].isna(), "abstract"].map(_clean)

# Save
os.makedirs("crawl_outputs", exist_ok=True)
citers.to_csv("crawl_outputs/inspire_citers.csv", index=False)
augmented_corpus.to_csv("crawl_outputs/corpus_augmented_inspire.csv", index=False)

print("Augmented corpus size:", augmented_corpus.shape)

"""## Cleaning and Assembling Final Corpus

Normalize abstracts, remove invalid entries, and prepare clean DataFrames (`corpus_patch`, `augmented_corpus`) for topic modeling.
"""

# =========================
# 6) CLEAN → LEMMATIZE → BUILD corpus_patch
# =========================
corpus_patch = augmented_corpus.dropna(subset=["title", "abstract"], how="any")

# Keep only rows with valid paper_key
corpus_patch = corpus_patch[
    corpus_patch["paper_key"].notna() &
    corpus_patch["paper_key"].astype(str).str.strip().ne("")
]

# Reset index properly
corpus_patch = corpus_patch.reset_index(drop=True)

"""### Saving the final dataset into a csv file."""

## save corpus_patch as a csv file
corpus_patch.to_csv("corpus_patch.csv", index=False)

"""### Using the corpus_patch dataframe so we dont have to build the dataframe again. Building is quite time consuming."""

# redefine corpus_patch from file
corpus_patch = pd.read_csv("corpus_patch.csv")

# drop entries with NaN in title/abstract
# =========================
corpus_patch = corpus_patch.dropna(subset=["title", "abstract"], how="any")

# Keep only rows with valid paper_key
corpus_patch = corpus_patch[
    corpus_patch["paper_key"].notna() &
    corpus_patch["paper_key"].astype(str).str.strip().ne("")
]

# Reset index properly
corpus_patch = corpus_patch.reset_index(drop=True)

n_rows, n_cols = corpus_patch.shape
print(f"The final dataset has {n_rows} rows and {n_cols} columns.\n")
display(corpus_patch)

"""## Wordcloud"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

text_blob = " ".join(corpus_patch["abstract"].dropna())
wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(text_blob)

plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Frequent Terms in Abstracts\n", fontsize=16)
plt.show()

text_blob = " ".join(corpus_patch["title"].dropna())
wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(text_blob)

plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Frequent Terms in titles\n", fontsize=16)
plt.show()

"""### Word Cloud Insights: Titles vs Abstracts

These word clouds visualize the most common terms found in:

- **Paper Titles**, where emphasis is on **particles, processes, and collisions**, such as *"top quark decay"*, or *"NNLO corrections at LHC"*.
- **Paper Abstracts**, which shift toward **interpretation and methodology**, using terms like *"results"*, *"measurement"*, *"present"*, *"cross section"* and *"Standard Model"*.
- Key recurring themes:
  - The **Higgs boson**, **top quark**, and **LHC** dominate both layers, confirming the centrality of electroweak precision physics.

#### **Comments from domain knowledge**

- Terms like `QCD`, `NNLO`, and `gluon fusion` emphasize ongoing efforts to refine theoretical predictions within the domain of `hadron colliders`.
- The presence of `dark matter`, `new physics`, and `MSSM` suggests active interest in Beyond Standard Model (BSM) searches.
- Although not dominant, terms such as `electroweak`, `resummation`, `threshold`, and `vector boson` highlight emerging or less-explored areas within the field. Research in these domains is ongoing, but progress is gradual due to the highly complex and computationally demanding nature of the underlying calculations.

Together, these clouds validate the thematic coverage of the corpus and guide topic modeling expectations toward precision collider phenomenology.

"""

# =========================
# 7) Inspect WordCloud words
# =========================
text_all = " ".join(corpus_patch["clean_abstract"].astype(str))
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text_all)

# List the words and frequencies
wordcloud_words = wordcloud.words_
for word, freq in wordcloud_words.items():
    print(f"{word}: {freq}")

nunique_words = len(wordcloud_words)
print(f"\nTotal unique words: {nunique_words}")

# Save unique words list for later use
unique_words = list(wordcloud_words.keys())

"""## Publication timelines"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 4))
sns.kdeplot(data=corpus_patch, x="year", fill=True, bw_adjust=1.2)
plt.title("Smoothed Density of Publications Over Time")
plt.xlabel("Year")
plt.ylabel("Density")
plt.tight_layout()
plt.show()

corpus_patch["year"] = pd.to_numeric(corpus_patch["year"], errors="coerce").astype("Int64")
corpus_patch = corpus_patch.sort_values("year")

# Add cumulative count
corpus_patch["cumulative_papers"] = range(1, len(corpus_patch)+1)

# Plot
plt.figure(figsize=(10, 4))
plt.plot(corpus_patch["year"], corpus_patch["cumulative_papers"], lw=2)
plt.xlabel("Year")
plt.ylabel("Total Papers So Far")
plt.title("Cumulative Number of Papers Over Time")
plt.grid(True)
plt.tight_layout()
plt.show()

# Count publications per year
pubs_per_year = corpus_patch["year"].value_counts().sort_index()

# Plot
plt.figure(figsize=(10, 5))
plt.plot(pubs_per_year.index, pubs_per_year.values, marker='o')
plt.title("Number of Publications per Year")
plt.xlabel("Year")
plt.ylabel("Number of Papers")
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Publication Timeline Analysis

These plots provide a detailed look at the temporal distribution of publications in my particle phenomenology corpus:

- The **annual paper count** (bottom) shows a sharp rise from 2000 onward, peaking around 2016–2018. This period aligns with heightened theoretical interest around the LHC's operational phases with the `Higgs boson` discovered in 2012.
- The **cumulative paper count** (middle) demonstrates the success of my recursive citation strategy, reaching over 5000 unique papers.
- The **smoothed density plot** (top) confirms that the bulk of research activity lies in the 2005–2020 window — making it the most meaningful period for a topic evolution study.

These visualizations help frame the historical context in which particle physics research — especially around Higgs, QCD, and electroweak theory — has matured and evolved.

## Starting NLP and ML usage to analyse topics
"""

# spaCy (same setup you used)
import spacy, re
from spacy.lang.en.stop_words import STOP_WORDS
from bertopic import BERTopic
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer

nlp = spacy.load("en_core_web_sm", disable=["parser","ner","textcat"])
nlp.max_length = max(1_000_000, corpus_patch["abstract"].astype(str).str.len().max() + 1000)

# Unigrams only
#unigram_vec = CountVectorizer(
#    ngram_range=(1, 1),
#    min_df=2,
#    max_df=0.95,
#    token_pattern=r"(?u)\b[^\W\d_][\w\-]{2,}\b"  # start with a letter, len>=3
#)

#bigram_vec = CountVectorizer(
#    ngram_range=(1, 2),   # include bigrams
#    min_df=2,
#    max_df=0.95,
#    lowercase=True,
#    token_pattern=r"(?u)\b[^\W\d_][\w\-]{2,}\b"  # still lets “higgs”, “boson” through
#)

"""## Defining stopwords, focus words (domain specific) and cleaning the corpus df for ML usage"""

domain_stop = {"however","therefore","thus","et","al","figure","fig","table","section","also","using","use","used","show","shown","result","results","approach","paper","work", "document class", "12pt", "mathrsfs", "oddsidemargin", "upgreek", "amsmath", }
custom_stop = set(STOP_WORDS) | domain_stop
WHITELIST = {"lhc","sm","bsm","ew","higgs","qcd","nlo","nnlo","eft","quark","top","bottom","cms","renormalization","tevatron","pdf","resummation"}

def spacy_lemmatize(text):
    if not isinstance(text,str) or text.strip().lower() in {"","nan","none"}: return ""
    try:
        text = re.sub(r"[^A-Za-z0-9\- ]+"," ", text)
        doc = nlp(text)
        keep = []
        for tok in doc:
            t = tok.text.lower()
            if t in WHITELIST: keep.append(t); continue
            if tok.is_stop or t in custom_stop: continue
            if tok.like_num or len(t) < 3: continue
            if tok.pos_ not in {"NOUN","VERB","ADJ","ADV","PROPN"}: continue
            lemma = tok.lemma_.lower().strip()
            if lemma and lemma not in custom_stop:
                keep.append(lemma)
        return " ".join(keep)
    except Exception:
        return ""

valid_mask = corpus_patch["abstract"].notna() & (corpus_patch["abstract"].astype(str).str.strip().str.lower() != "nan")
corpus_patch.loc[valid_mask, "clean_abstract"] = corpus_patch.loc[valid_mask, "abstract"].apply(spacy_lemmatize)
corpus_patch = corpus_patch[corpus_patch["clean_abstract"].astype(str).str.strip().str.len() > 0].copy()

corpus_patch["year"] = pd.to_numeric(corpus_patch["year"], errors="coerce").astype("Int64")
corpus_patch = corpus_patch.sort_values("year").reset_index(drop=True)

print(f"Combined cleaned corpus size: {len(corpus_patch)} rows")

# Treat 'nan'/'none'/'null' strings as missing
def is_bad_literal(x: str) -> bool:
    if not isinstance(x, str):
        return False
    return x.strip().lower() in {"nan", "none", "null"}

corpus_patch.loc[:, "abstract"] = corpus_patch["abstract"].apply(
    lambda x: None if (pd.isna(x) or is_bad_literal(str(x))) else str(x)
)

# spaCy lemmatizer you already defined: spacy_lemmatize()
# (Make sure it returns "" for empty/invalid input)
corpus_patch["clean_abstract"] = corpus_patch["abstract"].apply(spacy_lemmatize)

# Drop rows that are empty after cleaning
corpus_patch = corpus_patch[
    corpus_patch["clean_abstract"].apply(lambda s: isinstance(s, str) and s.strip() != "")
].copy()

# Extra guard: drop rows that are literally 'nan'/similar after cleaning
corpus_patch = corpus_patch[
    ~corpus_patch["clean_abstract"].str.fullmatch(r"\s*(nan|none|null)\s*", na=False, case=False)
].copy()

# ---------- 2) Extra stopwords for LaTeX cruft ----------
latex_stop = {
    "usepackage","documentclass","begin","end","document","figure","fig","table","caption",
    "section","subsection","subsubsection","abstract","bibliography","equation","ref","cite",
    "width","height","textbf","emph"
}
# If you had custom_stop earlier, extend it; else create:
try:
    custom_stop
except NameError:
    custom_stop = set()
custom_stop = set(custom_stop) | set(latex_stop)

# If your spacy_lemmatize uses custom_stop, great.
# Otherwise, quick post-filter: remove these tokens from cleaned text
def drop_stop_terms(txt: str) -> str:
    toks = txt.split()
    return " ".join(t for t in toks if t not in custom_stop)

corpus_patch["clean_abstract"] = corpus_patch["clean_abstract"].apply(drop_stop_terms)
corpus_patch = corpus_patch[
    corpus_patch["clean_abstract"].str.strip().str.len() > 0
].copy()

# Guard on year
corpus_patch["year"] = pd.to_numeric(corpus_patch["year"], errors="coerce").astype("Int64")
corpus_patch = corpus_patch[corpus_patch["year"].notna()].copy()

# --- 0) Build ONE aligned dataframe to use for everything ---
df_fit = corpus_patch.loc[:, ["clean_abstract", "year"]].copy()

# Clean/validate
df_fit["clean_abstract"] = df_fit["clean_abstract"].astype(str).str.strip()
df_fit["year"] = pd.to_numeric(df_fit["year"], errors="coerce")

mask = (df_fit["clean_abstract"].str.len() > 0) & (df_fit["year"].notna())
df_fit = df_fit.loc[mask].reset_index(drop=True)

# (Optional but recommended) cast to int to avoid float years
df_fit["year"] = df_fit["year"].astype(int)

# --- 1) Build inputs from THIS dataframe only ---
docs  = df_fit["clean_abstract"].tolist()
years = df_fit["year"].tolist()

if len(docs) == len(years):
  print("Lengths:", len(docs), len(years))  # must match
  print("Good to go.")
else:
  raise ValueError("Lengths don't match.")

print("Total rows:", len(corpus_patch))
print("Abstracts not null:", corpus_patch["abstract"].notna().sum())
print("Abstracts non-empty:", (corpus_patch["abstract"].astype(str).str.strip() != "").sum())

## Save updated corpus_path file as csv
corpus_patch.to_csv("corpus_patch_updated.csv", index=False)

"""## Topic Modeling with BERTopic

I have trained three separate BERTopic models using different parameters or random states. Analyze and compare the quality of resulting topics.

## Starting first BERTopic Run

## Using updated corpus patch dataset from above
"""

corpus_patch = pd.read_csv("corpus_patch_updated.csv")
display(corpus_patch.head())

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer

# docs = <the cleaned abstracts list>

# Phrase-aware vectorizer (unigrams + bigrams)
vectorizer = CountVectorizer(
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.95,
    token_pattern=r"(?u)\b[^\W\d_][\w\-]{2,}\b"  # start with a letter, len>=3
)

# Force 6 topics via KMeans (v0.17.x uses hdbscan_model= for custom clusterers)
kmeans_model = KMeans(n_clusters=6, random_state=42)

# Fit afresh with KeyBERT-inspired representations built-in
topic_model_1 = BERTopic(
    vectorizer_model=vectorizer,
    hdbscan_model=kmeans_model,            # <-- correct kwarg for 0.17.3
    representation_model=KeyBERTInspired(),# <-- phrases like "higgs boson"
    top_n_words=15,                         # show more terms per topic
    verbose=False
)

topics_1, probs_1 = topic_model_1.fit_transform(docs)

"""## Summarising first BERTopic run output"""

print("Docs empty?", len(docs) == 0)
print("Any NaN in years?", any(pd.isna(years)))
print("Unique topics (ex -1):", topic_model_1.get_topic_info().query("Topic >= 0").shape[0])

print("Max topic in assignments:", max(topics_1))
print("Shape of c_tf_idf_ matrix:", topic_model_1.c_tf_idf_.shape)
# print("Valid topic IDs:", valid_topic_ids) # Removed: valid_topic_ids is not defined here

"""The general topic is **Higgs boson**, **QCD**, **quark & gluon**, **loop**, **gauge theory** concepts and **collider** concepts."""

topic_model_1.visualize_barchart(top_n_topics=15).show()

"""### Comment on BERTopic: Top Terms for Each Topic

The above figure shows the top-ranked keywords in each topic from the BERTopic model trained on high-energy physics papers.

- **Topic 0** emphasizes quark-level dynamics and supersymmetric QCD.
- **Topic 1** highlights Higgs-related interactions, including neutral and heavy Higgs states (represents SM and BSM physics).  
- **Topic 2** pertains to BSM physics and fundamental theoretical developments in gauge symmetry.  
- **Topic 3** captures analytical formulations of loop computations, approximations and gauge-theory, linked to perturbative approaches.  
- **Topic 4** reflects experimental observables at the LHC — detector measurements, new-particle searches, and tau signatures.  
- **Topic 5** appears to reference specific experiments (e.g., Tevatron), indicating a dataset- or source-specific subcluster.

Together, these topics outline core directions in collider phenomenology, from theoretical precision studies to experimental detection and beyond–Standard Model Higgs analyses. Subsequent sections trace their temporal evolution to identify emerging and declining research fronts.
"""

topic_model_1.visualize_topics().show()

# Top words per topic
topic_model_1.get_topic(0)[:10]  # list of (word, score)

"""## Improving BERTopic
## Second Run: BERTopic (v0.17.x) with KMeans + KeyBERT-Inspired Labels

The below two cells build clean string inputs and year-end timestamps, then fits BERTopic using a phrase-aware representation (KeyBERTInspired) and fixed 6-topic KMeans clustering. After fitting, it optionally refines topic labels with MMR-based diversification and visualizes how topic prevalence evolves over time.
"""

from bertopic import BERTopic
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer

# --- 3) Build safe inputs ---
docs_list  = [str(d) for d in docs]   # plain strings
years_list = list(years)              # same length/order as docs

# years -> proper datetimes (use year-end to avoid construction issues)
years_dt = pd.to_datetime(pd.Series(years_list, dtype="int64").astype(str), format="%Y") + pd.offsets.YearEnd(0)
years_dt = years_dt.to_list()

# ---- 1) Robust imports for representations ----
rep_model = None
use_update = False
try:
    from bertopic.representation import KeyBERTInspired
    rep_model = KeyBERTInspired()
    use_update = True
except Exception:
    rep_model = None

# ---- 2) Vectorizer ----
vectorizer = CountVectorizer(
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.95,
    token_pattern=r"(?u)\b[^\W\d_][\w\-]{2,}\b"
)

# ---- 3) Clusterer ----
kmeans_model = KMeans(n_clusters=6, random_state=42)

# ---- 4) Fit ----
topic_model_2 = BERTopic(
    vectorizer_model=vectorizer,
    hdbscan_model=kmeans_model,
    representation_model=rep_model,
    top_n_words=15,
    verbose=False
)
topics_2, probs_2 = topic_model_2.fit_transform(docs_list)  # <-- use docs_list

# ---- 5) Optional label refinement ----
if use_update:
    try:
        topic_model_2.update_topics(
            docs_list,
            vectorizer,
            representation_model=rep_model,
            top_n_words=15,
            use_mmr=True,
            diversity=0.5
        )
    except TypeError:
        topic_model_2.update_topics(docs_list, vectorizer, representation_model=rep_model, top_n_words=15)
    except Exception:
        pass

# ---- 6) Inspect ----
info_2 = topic_model_2.get_topic_info()

# sanity
assert len(docs_list) == len(topic_model_2.topics_) == len(years_dt)

topic_model_2.visualize_barchart(top_n_topics=15).show()

"""### Comment on BERTopic: Top Terms for Each Topic

The above figure shows the top-ranked keywords in each topic from the second BERTopic model trained on high-energy physics papers.

- **Topic 0** focuses on QCD-driven collider phenomenology involving quarks and higgs processes at the LHC.  
- **Topic 1** centers on Higgs boson studies in collider environments, linking theoretical and experimental analyses.  
- **Topic 2** emphasizes perturbative QCD, Feynman amplitudes, and renormalization — fundamental components of theoretical particle physics.  
- **Topic 3** captures BSM Higgs-sector studies (e.g., MSSM) and their collider implications.  
- **Topic 4** also involves Higgs and LHC-related analyses, potentially corresponding to model-building or simulation-based work.  
- **Topic 5** represents QCD and quark-collider phenomenology, with mentions of proton-level processes relevant to hadron colliders.

Overall, these topics collectively span precision QCD, Higgs physics, and collider phenomenology, mirroring the main theoretical and experimental threads in high-energy physics research. Their temporal progression can help reveal how focus areas such as Higgs or QCD have evolved post-LHC era.

## Third run of BERTopic

This version of the BERTopic setup is more robust across installations. It tries to use KeyBERT-inspired phrase representations when available, but falls back safely to standard c-TF-IDF labeling if not. After fitting, it optionally refines topic labels using MMR (to diversify terms) - useful in recent BERTopic (≥0.17.x) builds. Compared to the simpler version, this one avoids version errors and tends to yield clearer, more varied topic descriptors.
"""

# ---- 1) Robust imports for representations (handle older/minimal installs) ----
rep_model = None
use_update = False
try:
    from bertopic.representation import KeyBERTInspired
    rep_model = KeyBERTInspired()
    use_update = True  # we can optionally refine labels after fit
except Exception:
    rep_model = None   # fall back to default c-TF-IDF labeling

# ---- 2) Vectorizer: include bigrams so phrases can surface even without YAKE ----
vectorizer = CountVectorizer(
    ngram_range=(1, 2),             # unigrams + bigrams
    min_df=2,
    max_df=0.95,
    token_pattern=r"(?u)\b[^\W\d_][\w\-]{2,}\b"
)

# ---- 3) Clusterer: force 6 topics with KMeans (v0.17.x uses hdbscan_model=) ----
kmeans_model = KMeans(n_clusters=6, random_state=42)

# ---- 4) Fit afresh (representation_model used if available) ----
topic_model_3 = BERTopic(
    vectorizer_model=vectorizer,
    hdbscan_model=kmeans_model,     # correct kwarg for 0.17.x
    representation_model=rep_model, # None -> c-TF-IDF labels; KeyBERTInspired -> phrase-aware
    top_n_words=15,
    verbose=False
)
topics_3, probs_3 = topic_model_3.fit_transform(docs_list)

# ---- 5) OPTIONAL: refine labels with MMR if your version supports it ----
# (update_topics in 0.17.x uses positional args; use_mmr is available in many builds)
if use_update:
    try:
        topic_model_3.update_topics(
            docs,                   # positional: documents
            vectorizer,             # positional: vectorizer
            representation_model=rep_model,
            top_n_words=15,
            use_mmr=True,           # diversify terms if supported
            diversity=0.5
        )
    except TypeError:
        # Some 0.17 builds don’t accept use_mmr/diversity here; fall back silently
        topic_model_3.update_topics(docs, vectorizer, representation_model=rep_model, top_n_words=15)
    except Exception:
        pass  # keep the labels from initial fit if update isn’t supported

# ---- 6) Inspect a couple topics ----
info_3 = topic_model_3.get_topic_info()
# For detailed terms: topic_model.get_topic(k)

topic_model_3.visualize_barchart(top_n_topics=15).show()

"""### Comment on BERTopic: Top Terms for Each Topic

The above figure shows the top-ranked keywords in each topic from the third BERTopic model trained on high-energy physics papers.

- **Topic 0** focuses on QCD and collider-based analyses, emphasizing quark interactions and Higgs-related processes at the LHC.  
- **Topic 1** centers on Higgs boson studies in collider environments.
- **Topic 2** reflects beyond–Standard Model Higgs physics (e.g., MSSM extensions) and gauge-theory considerations.  
- **Topic 3** highlights collider operations and experimental contexts, with terms like “proton” and “CERN” linking to LHC infrastructure.  
- **Topic 4** captures fundamental theoretical precision topics — renormalization, anomalous couplings, and QCD corrections.  
- **Topic 5** emphasizes analytical and computational aspects of QCD, including Feynman integrals and numerical methods.

Together, these topics delineate a balanced spectrum of collider phenomenology — from theoretical QCD and Higgs modeling to experimental LHC studies and computational techniques — forming the core landscape of modern high-energy physics research.
"""

topic_model_3.visualize_topics().show()

"""### Topics over Time for the three BERTopic runs"""

# --- Topics-over-time (v0.17.x) ---
tot_1 = topic_model_1.topics_over_time(docs_list, years_dt)
topic_model_1.visualize_topics_over_time(tot_1, top_n_topics=6).show()

"""### Comment

1. **Topic 0** (`cross quark, mixed qcd, transverse`) rises steadily after 2005 and peaks post-2015, reflecting intensified efforts in higher-order QCD corrections and precision collider phenomenology.  
2. **Topic 1** (`neutral higgs, decay quark, higgs doublet`) shows pronounced growth between 2010–2015, aligning with post-discovery Higgs studies. After a brief decline, recent fluctuations suggest renewed interest, possibly linked to precision Higgs coupling analyses.  
3. **Topic 2** (`gev higgs, supersymmetric standard, heavy higgs`) follows a similar trend but fades after 2017, indicating diminishing emphasis on MSSM-type extensions within collider contexts.  
4. **Topic 3** (`massless qcd, loop qcd, gauge theory`) remains steady at moderate frequency, marking a mature yet persistent area in perturbative QCD and theoretical foundations.  
5. **Topic 4** (`measurement quark, detector lhc, new particle`) expands during 2008–2015 in sync with the LHC’s early data-taking era, then stabilizes. Although currently lower, it may rise again with Run 3 and upcoming collider programs.  
6. **Topic 5** (`tevatron lhc, strong coupling, lattice qcd`) peaks around 2010, consistent with late-Tevatron activity and concurrent lattice QCD precision efforts.

### **Interpretation**

The temporal trends reveal a clear shift from foundational and lattice QCD toward Higgs-centered and beyond-Standard-Model collider studies, closely following LHC milestones.  
After 2015, research consolidates around precision Higgs phenomenology, while supersymmetric explorations gradually recede — marking a transition toward data-driven and high-precision frontiers in particle physics.
"""

# --- Topics-over-time (v0.17.x) ---
tot_2 = topic_model_2.topics_over_time(docs_list, years_dt)
topic_model_2.visualize_topics_over_time(tot_2, top_n_topics=6).show()

"""### Comment

1. **Topic 0** (`qcd, higgs, lhc, quark`) rises sharply after 2005 and peaks around 2015, reflecting the consolidation of collider-based QCD and Higgs analyses during the main LHC data-taking era.  
2. **Topic 1** (`higgs, lhc, quark, collider`) tracks a similar trend, maintaining strong prominence post-2012 — consistent with sustained interest in Higgs production and decay phenomenology.  
3. **Topic 2** (`qcd, quark, feynman, amplitude`) grows steadily from the early 2000s to 2015, aligning with advances in higher-order perturbative calculations and theoretical formalisms.  
4. **Topic 3** (`higgs, lhc, collider, quark`) shows pronounced activity between 2010–2018, corresponding to experimental and phenomenological studies surrounding the Higgs discovery.  
5. **Topic 4** (`higgs, lhc, mml, quark`) peaks around 2012–2016, possibly capturing MSSM or model-extension studies closely tied to the Higgs sector.  
6. **Topic 5** (`qcd, lhc, quark, collider`) maintains moderate frequency across the timeline, representing enduring interest in collider phenomenology and partonic dynamics.

### **Interpretation**

This evolution mirrors the LHC-driven transformation of the field: early theoretical QCD studies gave way to intense Higgs-focused collider research following 2012. The peaks around 2015 correspond to the consolidation of precision Higgs measurements and advanced QCD modeling, while recent moderation suggests a mature, data-rich phase emphasizing precision and cross-correlation between theory and experiment.
"""

# --- Topics-over-time (v0.17.x) ---
tot_3 = topic_model_3.topics_over_time(docs_list, years_dt)
topic_model_3.visualize_topics_over_time(tot_3, top_n_topics=6).show()

"""### Comment

1. **Topic 0** (`qcd, lhc, quark, higgs`) rises sharply after 2005, peaking around 2015 — reflecting the integration of QCD precision calculations into LHC Higgs analyses.  
2. **Topic 1** (`higgs, lhc, collider, quark`) shows dominant growth from 2010–2017, corresponding to the Higgs discovery and the surge in collider phenomenology.  
3. **Topic 2** (`higgs, lhc, quark, mssm`) mirrors Topic 1 but declines after 2018, signaling reduced focus on MSSM-specific Higgs models as the field shifted toward data-driven approaches.  
4. **Topic 3** (`lhc, quark, proton, collider`) peaks near 2015 and reflects core LHC proton–proton studies and collider operations; its persistence suggests ongoing relevance to Run 3 and future high-luminosity phases.  
5. **Topic 4** (`qcd, quark, renormalization, anomalous`) maintains moderate and steady activity, representing foundational theoretical work in perturbative QCD and effective couplings.  
6. **Topic 5** (`qcd, feynman, integration, quark`) increases gradually from 2005 to 2015, linked to developments in loop integration and precision theory methods, before stabilizing.

### **Interpretation**

The time evolution highlights a coordinated shift from QCD formalism toward Higgs-focused collider studies, tightly synchronized with LHC operations.
Post-2012 peaks align with the Higgs discovery, after which MSSM-related research declines while precision and methodological QCD work continues — marking a mature, data-driven phase emphasizing high-precision theoretical predictions and collider synergy.

### Summary of the Topics Over Time: Evolution of Research Themes

The three `topics_over_time` plots collectively depict the temporal evolution of dominant research themes in the high-energy physics corpus from roughly **1995 to 2025**, as derived from successive BERTopic models. Together, they trace how the community’s focus has evolved from broad collider studies and QCD formalisms to precision-era Higgs and BSM analyses.

Across all three BERTopic runs, the most persistent and high-frequency clusters revolve around **QCD**, **Higgs boson physics**, and **LHC collider phenomenology**, with frequent co-occurrence of **`quark`, `collider`, `lhc`, `higgs`, and `qcd`** terms.  
1. These topics exhibit rapid growth after **2005**, peaking between **2012–2020**, directly aligning with the **Higgs boson discovery** and **LHC Run II**. The trends capture both theoretical advancements (higher-order QCD, loop amplitudes, renormalization) and experimental milestones (detector studies, proton–proton analyses).

2. Despite minor variations in topic composition across models, strong semantic overlap indicates robust convergence: all runs highlight a dominant **Higgs–QCD–collider triad**, with secondary contributions from **MSSM/BSM Higgs sectors**, **lattice and computational QCD**, and **precision measurement techniques**.  
Post-2015, these themes consolidate toward **precision Higgs coupling**, **effective field theories**, and **data-intensive collider analyses**.

Overall, the temporal dynamics reflect a clear research trajectory — from early foundational theory and collider setup to mature, data-driven, and computationally advanced **precision collider phenomenology** — mirroring the real-world evolution of particle physics in the LHC era.
"""

topic_model_1.get_topic_info()

topic_model_2.get_topic_info()

topic_model_3.get_topic_info().head(10)

topic_model_1.get_topic(0)[:10]  # list of (word, score)

topic_model_2.get_topic(0)[:10]  # list of (word, score)

# Top words per topic
topic_model_3.get_topic(0)[:10]  # list of (word, score)

"""### Comment on Extracted Top Words per Topic

The above extracted word lists provide insight into the semantic stability and interpretability of the dominant cluster.

- **Model 1:**  
  Top words (`cross quark`, `mixed qcd`, `quark transverse`, `annihilation`, `supersymmetric standard`, `prediction higgs`, `tevlatron lhc`) highlight a **QCD–collider hybrid theme**, mixing **quark-level processes**, **supersymmetry**, and **Tevatron/LHC** context.  
  → Represents early collider theory with BSM (SUSY) overlap.

- **Model 2:**  
  Top words (`qcd`, `higgs`, `lhc`, `quark`, `collider`, `gluon`, `coupling`) show a **more cohesive collider–Higgs–QCD topic**, dominated by **precision-theory** and **LHC-scale phenomena**.  
  → Indicates improved topic focus and less lexical noise.

- **Model 3:**  
  Top words (`qcd`, `lhc`, `higgs`, `quark`, `gluon`, `electroweak`, `boson`, `coupling`) refine further into a **central high-energy phenomenology cluster**, cleanly representing **Higgs–QCD–EW unification themes**.  
  → Suggests semantic convergence and topic stabilization across models.

**Overall Observation:**  
There is clear thematic consistency — all three models’ top topics are anchored in **QCD + Higgs + collider physics**, evolving from mixed collider/SUSY language (Model 1) to a more **focused and interpretable representation** (Model 3). This progression indicates effective refinement through preprocessing, dimensionality tuning, or embedding quality improvements, yielding stronger topical coherence in later runs.

# Assign BERTopic Results Back to Original DataFrame

This cell refits BERTopic on the selected abstracts (clean_abstract) in a fixed order, refines topic labels if supported, and writes the resulting topic IDs back to the corresponding rows of the original dataset (corpus_patch or augmented_corpus). It ensures one-to-one alignment between fitted documents and DataFrame rows, then updates the global DataFrame with a new "Topic" column and prints a brief summary.
"""

# 0) Source df
df_src = augmented_corpus if "augmented_corpus" in globals() else corpus_patch

# 1) Build a conservative base mask
ca = df_src["clean_abstract"].astype(str)
mask_base = (
    ca.notna() &
    ca.str.strip().ne("") &
    ~ca.str.strip().str.lower().isin(["none","nan"])
)

# 2) If you saved the exact rows used for fitting, prefer those:
fit_idx_candidates = []
for var in ["fit_idx_r1", "fit_indices_r1", "used_idx_r1", "docs_used_idx"]:
    if var in globals():
        fit_idx_candidates = list(globals()[var])
        break

fit_keys_candidates = []
for var in ["fit_doc_keys_r1", "used_doc_keys_r1", "docs_used_keys_r1"]:
    if var in globals():
        fit_keys_candidates = list(globals()[var])
        break

def assign_topics_with_index(index_list):
    # Ensure all are present in current df
    missing = [i for i in index_list if i not in df_src.index]
    if missing:
        raise ValueError(f"{len(missing)} fit indices no longer present in df.")
    idx = pd.Index(index_list)
    if len(idx) != len(topics_1):
        raise ValueError(f"Saved fit index length {len(idx)} != topics length {len(topics_1)}.")
    df_src["Topic_r1"] = pd.Series(topics_1, index=idx, dtype="Int64").reindex(df_src.index)
    df_src["Topic_r2"] = pd.Series(topics_2, index=idx, dtype="Int64").reindex(df_src.index)
    df_src["Topic_r3"] = pd.Series(topics_3, index=idx, dtype="Int64").reindex(df_src.index)

def assign_topics_with_key(key_list, key_col):
    # Map keys -> current index
    if key_col not in df_src.columns:
        raise ValueError(f"Column {key_col} not in df.")
    # Build a lookup from key to index (first occurrence)
    key_to_idx = df_src.reset_index().drop_duplicates(subset=[key_col]).set_index(key_col)["index"].to_dict()
    missing = [k for k in key_list if k not in key_to_idx]
    if missing:
        raise ValueError(f"{len(missing)} fit keys not found in df[{key_col}].")
    idx = pd.Index([key_to_idx[k] for k in key_list])
    if len(idx) != len(topics_1):
        raise ValueError(f"Key-mapped index length {len(idx)} != topics length {len(topics_1)}.")
    df_src["Topic_r1"] = pd.Series(topics_1, index=idx, dtype="Int64").reindex(df_src.index)
    df_src["Topic_r2"] = pd.Series(topics_2, index=idx, dtype="Int64").reindex(df_src.index)
    df_src["Topic_r3"] = pd.Series(topics_3, index=idx, dtype="Int64").reindex(df_src.index)

# Try exact index alignment first
done = False
if fit_idx_candidates:
    assign_topics_with_index(fit_idx_candidates)
    done = True
elif fit_keys_candidates:
    # Try several likely key columns
    for key_col in ["uid","paper_id","arxiv_id","doi","sha","doc_key"]:
        try:
            assign_topics_with_key(fit_keys_candidates, key_col)
            done = True
            break
        except Exception:
            pass

if not done:
    # 3) Reconstruct a *stricter* mask closer to training-time filters
    #    - drop duplicate abstracts
    #    - drop very short abstracts (e.g., < 5 tokens)
    #    - drop common placeholder phrases
    placeholders = [
        "no abstract available", "abstract unavailable", "not available",
        "this paper has been withdrawn", "retracted", "placeholder abstract"
    ]
    ph_mask = ca.str.lower().str.contains("|".join(map(repr, placeholders)).replace("'", ""), regex=True)
    token_mask = ca.str.split().map(lambda x: len(x) if isinstance(x, list) else 0) >= 5
    dedup_mask = ~ca.str.strip().str.lower().duplicated(keep="first")

    mask_fit_like = mask_base & token_mask & dedup_mask & ~ph_mask
    df_used = df_src[mask_fit_like].copy()
    idx = df_used.index

n_topics = len(topics_1)

# 1) Prefer an exact-length index if available; otherwise fall back intelligently
cand_indices = []

# (A) If your "fit-like" idx exists *and* matches n_topics, use it
try:
    idx_fit_like = df_src[mask_fit_like].index  # only if you created mask_fit_like earlier
    if len(idx_fit_like) == n_topics:
        cand_indices.append(idx_fit_like)
except NameError:
    pass

# (B) If base_valid matches n_topics (your case: 5344 == 5344), use it
idx_base = df_src[mask_base].index
if len(idx_base) == n_topics:
    cand_indices.append(idx_base)

# (C) If base_valid has more, truncate deterministically to n_topics
if len(idx_base) > n_topics:
    cand_indices.append(idx_base[:n_topics])

# (D) If base_valid has fewer, that's a hard stop—can’t safely align
if not cand_indices:
    raise ValueError(
        f"Cannot align: topics={n_topics}, base_valid={len(idx_base)}. "
        "You likely trained with a different filter or removed rows since training."
    )

# Pick the first viable candidate
idx = cand_indices[0]

# 2) Assign topics back aligned to idx
df_src["Topic_r1"] = pd.Series(topics_1, index=idx, dtype="Int64").reindex(df_src.index)
df_src["Topic_r2"] = pd.Series(topics_2, index=idx, dtype="Int64").reindex(df_src.index)
df_src["Topic_r3"] = pd.Series(topics_3, index=idx, dtype="Int64").reindex(df_src.index)

# 3) Publish
if "augmented_corpus" in globals():
    augmented_corpus = df_src
else:
    corpus_patch = df_src

print(df_src[["Topic_r1","Topic_r2","Topic_r3"]].notna().sum().to_dict())

# --- Remove entries missing title, abstract, or all topic assignments ---
df_src_cleaned = (
    df_src
    # 1) Drop rows with missing or empty titles/abstracts
    .dropna(subset=["title", "abstract"])
    .loc[lambda d: d["abstract"].astype(str).str.strip().ne("")]

    # 2) Keep rows where at least one topic column is not NA
    .loc[lambda d: d[["Topic_r1", "Topic_r2", "Topic_r3"]].notna().any(axis=1)]
    .copy()
)

print("Before:", len(df_src))
print("After cleaning:", len(df_src_cleaned))

display(df_src_cleaned.tail())

# save df_src_cleaned in a csv file
df_src_cleaned.to_csv("df_src_cleaned.csv", index=False)

def preview_bad_topics(model, top_n=10, max_show=5):
    bad = []
    for k, words in (model.get_topics() or {}).items():
        if k == -1:
            continue
        toks = [w for w, _ in (words or [])[:top_n]]
        ok = all(isinstance(w, str) and w.strip() for w in toks)
        if not ok:
            bad.append((k, toks))
        if len(bad) >= max_show:
            break
    return bad

print("Example problematic topics:", preview_bad_topics(topic_model_2))

# === Rank 3 BERTopic runs by coherence (tie-break: diversity) & show NMI ===
from sklearn.metrics import normalized_mutual_info_score

# ---------- 0) Pick source DataFrame ----------
if "df_src_cleaned" in globals():
    df_src = df_src_cleaned
elif "augmented_corpus" in globals():
    df_src = augmented_corpus
else:
    df_src = corpus_patch

# ---------- 1) Build ONE mask used for docs and topic arrays ----------
ca_str = df_src["clean_abstract"].astype(str)
mask_eval = (
    df_src["clean_abstract"].notna() &
    ca_str.str.strip().ne("") &
    ~ca_str.str.strip().str.lower().isin(["none", "nan"])
)

valid_docs = ca_str[mask_eval].tolist()

def topics_array(col):
    if col not in df_src.columns:
        raise KeyError(f"Missing column: {col}. Did you assign Topic_* back to df_src?")
    return (df_src.loc[mask_eval, col]
            .astype("Int64")
            .astype("float")
            .fillna(-1)
            .astype(int)
            .to_numpy())

# Use precomputed arrays if they exist AND match the mask length; else derive from columns
n_mask = int(mask_eval.sum())
t1 = topics_1 if ("topics_1" in globals() and len(topics_1) == n_mask) else topics_array("Topic_r1")
t2 = topics_2 if ("topics_2" in globals() and len(topics_2) == n_mask) else topics_array("Topic_r2")
t3 = topics_3 if ("topics_3" in globals() and len(topics_3) == n_mask) else topics_array("Topic_r3")

assert len(valid_docs) == len(t1) == len(t2) == len(t3), \
    f"Length mismatch: docs={len(valid_docs)}, t1={len(t1)}, t2={len(t2)}, t3={len(t3)}, mask={n_mask}"

# ---------- 2) Metrics helpers ----------
def topic_diversity_manual(model, top_n=10):
    topics = model.get_topics() or {}
    all_words = []
    for k, words in topics.items():
        if k == -1 or not words:
            continue
        all_words.extend([w for w, _ in words[:top_n]])
    return np.nan if not all_words else len(set(all_words)) / len(all_words)

def coherence_gensim(model, docs, top_n=10, min_tokens_per_topic=2):
    """Safe C_v coherence; returns np.nan if gensim unavailable or topics are unusable."""
    try:
        from gensim.corpora import Dictionary
        from gensim.models.coherencemodel import CoherenceModel
    except Exception:
        return np.nan

    # Tokenize docs
    tokenized = []
    for d in docs:
        if isinstance(d, str):
            toks = [t for t in d.lower().split() if t]
            if toks:
                tokenized.append(toks)
    if not tokenized:
        return np.nan

    # Collect & sanitize topic words
    topics = model.get_topics() or {}
    topic_words = []
    for k, words in topics.items():
        if k == -1 or not words:
            continue
        toks = [str(w).strip() for (w, _) in words[:top_n] if w is not None]
        toks = [t for t in toks if t]
        if len(toks) >= min_tokens_per_topic:
            topic_words.append(toks)
    if not topic_words:
        return np.nan

    dictionary = Dictionary(tokenized)
    if len(dictionary) == 0:
        return np.nan

    try:
        cm = CoherenceModel(
            topics=topic_words,
            texts=tokenized,
            dictionary=dictionary,
            coherence="c_v",
        )
        return float(cm.get_coherence())
    except ValueError:
        return np.nan

def size_stats(model):
    info = model.get_topic_info()
    info = info[info["Topic"] != -1]
    if info.empty:
        return np.nan, np.nan, np.nan
    return float(info["Count"].mean()), float(info["Count"].std()), int(info["Count"].max())

# ---------- 3) Gather models (skip any missing) ----------
models = {}
if "topic_model_1" in globals(): models["r1"] = topic_model_1
if "topic_model_2" in globals(): models["r2"] = topic_model_2
if "topic_model_3" in globals(): models["r3"] = topic_model_3

# ---------- 4) Compute per-run metrics ----------
rows = []
for name, model in models.items():
    div = topic_diversity_manual(model, top_n=10)
    coh = coherence_gensim(model, valid_docs, top_n=10)
    mean_sz, std_sz, max_sz = size_stats(model)
    rows.append({
        "run": name,
        "coherence_c_v": coh,
        "diversity@10": div,
        "size_mean": mean_sz,
        "size_std": std_sz,
        "size_max": max_sz
    })

metrics_df = (pd.DataFrame(rows)
              .set_index("run")
              .sort_values(by=["coherence_c_v", "diversity@10"], ascending=False)) if rows else pd.DataFrame()

print("=== Model metrics (higher coherence & diversity are better) ===")
try:
    display(metrics_df)
except Exception:
    print(metrics_df)

# ---------- 5) Agreement between runs (NMI on SAME mask) ----------
print("\n=== Normalized Mutual Information (topic assignments on valid_docs) ===")
print("NMI(r1, r2):", normalized_mutual_info_score(t1, t2))
print("NMI(r1, r3):", normalized_mutual_info_score(t1, t3))
print("NMI(r2, r3):", normalized_mutual_info_score(t2, t3))

# ---------- 6) Pick winner ----------
if not metrics_df.empty:
    winner = metrics_df.index[0]
    print(f"\nSuggested winner by coherence/diversity: {winner}")
else:
    print("\nNo models available to score (topic_model_* missing?).")

"""### Comment on Model Comparison and Selection

The above table summarizes BERTopic model metrics across three runs (`r1`, `r2`, `r3`), evaluated by **coherence**, **diversity**, and **topic assignment similarity** (via Normalized Mutual Information, NMI).

- **Coherence (`c_v`)** measures the semantic interpretability of topics — higher is better.  
  → `r2` achieves the highest coherence (**0.578**), indicating more internally consistent topics compared to `r3` (0.557).  
  → `r1` lacks a valid coherence score (NaN), likely due to instability or small sample overlap.

- **Diversity@10** captures lexical variety among top words in different topics — higher values mean less redundancy.  
  → `r3` shows slightly higher diversity (**0.63**) than `r2` (**0.53**), suggesting marginally broader vocabulary coverage.  
  However, excessive diversity can sometimes fragment semantically similar clusters, so a moderate trade-off is desirable.

- **Topic size distribution** (`size_mean`, `size_std`, `size_max`) is nearly identical across models, implying comparable clustering structure — no run dominates purely by topic count or spread.

- **NMI scores** between runs show **strong alignment** (`~0.68–0.76`), confirming overall stability of topic assignments.  
  → The closest alignment is between `r1` and `r3` (0.76), while `r2` and `r3` are moderately consistent (0.68).  
  → These values imply that all models capture similar conceptual groupings, differing mainly in topic clarity.

### **Interpretation**

Although `r3` offers slightly higher lexical diversity, `r2` achieves the **best balance** between:
- **High coherence (interpretability)**,  
- **Stable topic sizes**, and  
- **Reasonable diversity**.

Combined with your **qualitative inspection** (Model 2’s top words forming a clear, physics-meaningful “QCD–Higgs–Collider” cluster), this quantitatively supports the selection of **Model 2 (r2)** as the most coherent and thematically stable version of the topic model.

✅ **Conclusion:** Model 2 (r2) is the justified winner — it yields the most interpretable, semantically cohesive, and physically meaningful topic structure.
"""

# 1) Assign final topic column dynamically based on the winning run
df_src["Topic_final"] = df_src[f"Topic_{winner}"]

# 2) Choose the corresponding model explicitly (based on your 'winner')
final_model = {
    "r1": topic_model_1,
    "r2": topic_model_2,
    "r3": topic_model_3
}[winner]

# 3) Save model and annotated corpus
final_model.save("bertopic_final_model")
df_src.to_csv("corpus_with_final_topics.csv", index=False)

print(f"Saved final model ({winner}) and corpus_with_final_topics.csv")

# Always get topic info fresh from the final model
final_model = {
    "r1": topic_model_1,
    "r2": topic_model_2,
    "r3": topic_model_3
}[winner]

info = final_model.get_topic_info().copy()
info["Topic"] = info["Topic"].astype(int)

print(f"Showing head of topic info for {winner}:")
print(info.head())

"""## Time-Series Analysis"""

import warnings
warnings.filterwarnings("ignore")  # optional, silences statsmodels messages

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request, os
import fitz
import re, time, html, requests
from collections import defaultdict
from tqdm import tqdm
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
from bertopic import BERTopic
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from pathlib import Path

# Load the previously saved final model
final_model = BERTopic.load("bertopic_final_model")
df_src = pd.read_csv("corpus_with_final_topics.csv")

info = final_model.get_topic_info().copy()
info["Topic"] = info["Topic"].astype(int)

# Optional: create topic label mapping
topic_labels = {
    row["Topic"]: row["Name"].replace("_", " ").title()
    for _, row in info.iterrows() if row["Topic"] != -1
}

# BERTopic configuration snapshot (for reproducibility)
try:
    params = final_model.get_params()
    print("BERTopic parameters:")
    for k, v in params.items():
        print(f"- {k}: {v}")
except Exception as e:
    print("Could not print BERTopic params:", e)

## columns in df_drc
df_src.columns

display(df_src.head())

# --- 0) Restore topics and years from df_src ---
topics = df_src["Topic_final"].astype(int).to_numpy()
years = df_src["year"].astype(int).to_numpy()

# --- 1) Yearly topic counts (your original Step 1) ---
df = pd.DataFrame({"year": years, "topic": topics})
df = df[df["topic"] != -1]  # remove outliers/noise

topic_ts = (
    df.groupby(["year", "topic"])
      .size()
      .reset_index(name="count")
      .sort_values(["topic", "year"])
)
display(topic_ts.head())

# --- 2) Topic evolution in 5-year bins with smoothing (your original Step 2) ---
# Ensure int years
df["year"] = df["year"].astype(int)

# Create 5-year bins
df["YearBin"] = (df["year"] // 5) * 5

# Count number of publications per (YearBin, Topic)
topic_trends = df.groupby(["YearBin", "topic"]).size().unstack(fill_value=0)

# Optional: remove bins with too few total papers (threshold = 5)
topic_trends = topic_trends[topic_trends.sum(axis=1) >= 5]

# Normalize each bin to proportions
topic_trends = topic_trends.div(topic_trends.sum(axis=1), axis=0)

# Smooth trends with a rolling average (window = 2 bins)
topic_trends_smoothed = topic_trends.rolling(window=2, min_periods=1, center=True).mean()

# Optional: relabel columns with human-friendly topic names if available
# (Will only rename topics present as columns.)
if isinstance(topic_labels, dict):
    rename_map = {col: topic_labels.get(col, col) for col in topic_trends_smoothed.columns}
    topic_trends_smoothed = topic_trends_smoothed.rename(columns=rename_map)

import matplotlib.pyplot as plt
import seaborn as sns

# Use Seaborn color palette for clarity
palette = sns.color_palette("tab10", n_colors=len(topic_trends_smoothed.columns))

plt.figure(figsize=(12, 6))

for i, col in enumerate(topic_trends_smoothed.columns):
    # Get a clean label (human-readable if topic_labels available)
    label = topic_labels.get(col, f"Topic {col}") if isinstance(topic_labels, dict) else f"Topic {col}"
    plt.plot(
        topic_trends_smoothed.index,
        topic_trends_smoothed[col],
        label=label,
        color=palette[i % len(palette)],
        linewidth=2,
        marker="o",
        markersize=5
    )

plt.title("Topic Evolution Over Time (5-Year Bins, Smoothed)", fontsize=14, fontweight="bold")
plt.xlabel("Year (5-Year Bins)", fontsize=12)
plt.ylabel("Proportion of Total Publications", fontsize=12)
plt.legend(title="Topics", bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=9)
plt.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

"""### Topic Evolution (5-Year Smoothed)

The above plot illustrates the temporal evolution of topic proportions in the best-performing BERTopic model (`r2`), aggregated into 5-year bins and normalized to show their relative contribution to total publications.

- **Topic 0 (QCD–Higgs–LHC–Quark)** dominates throughout the timeline, maintaining over 50% of publications up to the mid-2000s before gradually declining. This reflects the early centrality of QCD and Higgs studies in collider phenomenology.  
- **Topic 1 (Higgs–LHC–Quark–Collider)** gains strength after 2010, peaking around 2015 — corresponding to the post-Higgs discovery era and precision collider analyses.  
- **Topic 2 (QCD–Quark–Feynman–Amplitude)** rises from the late 1990s and remains steady, indicating enduring activity in theoretical and computational QCD.  
- **Topic 3 (Higgs–LHC–Collider–Quark)** shows fluctuations tied to LHC data-taking phases, reinforcing its experimental character.  
- **Topic 4 (Higgs–LHC–MML–Quark)** exhibits gradual growth after 2005, possibly reflecting increasing focus on model-specific Higgs or EFT frameworks.  
- **Topic 5 (QCD–LHC–Quark–Collider)** remains stable at lower frequency. This can be due to the persistent theoretical backbone of collider analyses or the theoretical focus of this project in extracting databases.

### **Interpretation**

Overall, the smoothed temporal trends highlight a **transition from QCD-dominated to Higgs-centric research**, aligned with LHC operational milestones and major discoveries. After 2015, topic shares converge, suggesting a **mature, diversified research landscape** balancing QCD methodology, Higgs precision measurements, and collider phenomenology.
"""

# df must have per-document assignments
# df = pd.DataFrame({"year": years, "topic": topics})
df = df[df["topic"] != -1].copy()
df["year"] = pd.to_numeric(df["year"], errors="coerce").astype("Int64")
df = df.dropna(subset=["year"]).copy()
df["year"] = df["year"].astype(int)

# 1) Wide COUNT matrix: rows=year, cols=topic
counts_wide = (
    df.groupby(["year", "topic"])
      .size()
      .unstack(fill_value=0)
      .sort_index()
)

# Ensure continuous year index
full_years = range(counts_wide.index.min(), counts_wide.index.max() + 1)
counts_wide = counts_wide.reindex(full_years, fill_value=0)

# 2) Row-wise SHARES (each year sums to 1; protect division by zero)
row_sums = counts_wide.sum(axis=1).replace(0, 1)
shares_wide = counts_wide.div(row_sums, axis=0)

# 3) Use a proper yearly DateTimeIndex (end-of-year stamps)
shares_wide.index = pd.to_datetime(shares_wide.index.astype(int), format="%Y") + pd.offsets.YearEnd(0)

# <-- This is your topic_series (use shares_wide OR counts_wide depending on what you want to model)
topic_series = shares_wide

# Quick sanity checks
print("Total counts:", counts_wide.values.sum())
print("Any nonzero rows? ", (counts_wide.sum(axis=1) > 0).any())

print(f"Dataset starts from year {df["year"].min()} and continues till {df["year"].max()}.")
print("Topics:", sorted(df["topic"].unique()))
print("Nonzero total:", counts_wide.values.sum())
print("Nonzero years:", (counts_wide.sum(axis=1) > 0).sum(), "of", len(counts_wide))

print(topic_series.head(), "\n\n", topic_series.tail())

# Normalized, nicely labeled topic evolution plot for `topic_series`
# Assumes:
# - topic_series: index = years (or bins), columns = topic IDs (e.g., ints)
# - topic_labels: dict {topic_id: "Readable Name"}

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns
import numpy as np
import pandas as pd

# 1) Normalize rows to proportions (each year sums to 1)
topic_series_norm = topic_series.div(topic_series.sum(axis=1).replace(0, np.nan), axis=0).fillna(0)

# 2) Build human-friendly labels without renaming the DataFrame
def pretty_label(col):
    # If labels exist and this col has one, use it; otherwise fall back
    if isinstance(topic_labels, dict) and col in topic_labels:
        return topic_labels[col]
    # Try to coerce numeric-looking strings to int for lookup
    try:
        col_int = int(col)
        return topic_labels.get(col_int, f"Topic {col}")
    except Exception:
        return f"Topic {col}"

labels = [pretty_label(col) for col in topic_series_norm.columns]

# 3) Plot
palette = sns.color_palette("tab20", n_colors=len(topic_series_norm.columns))

plt.figure(figsize=(12, 6))
for i, col in enumerate(topic_series_norm.columns):
    plt.plot(
        topic_series_norm.index,
        topic_series_norm[col],
        label=labels[i],
        color=palette[i % len(palette)],
        linewidth=2,
        marker="o",
        markersize=4
    )

plt.title("Topic Evolution Over Time (normalized shares)", fontsize=14, fontweight="bold")
plt.xlabel("Year", fontsize=12)
plt.ylabel("Proportion of total publications", fontsize=12)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))  # show as %
plt.grid(True, linestyle="--", alpha=0.5)
plt.legend(title="Topics", bbox_to_anchor=(1.02, 1), loc="upper left", fontsize=9)
plt.tight_layout()
plt.show()

"""#### The timeline before 1990 is not very effective/relavnt in this study. So, lets plot without it."""

topic_series_norm.index

## Plotting for a shorter x-range
plt.figure(figsize=(12, 6))
for i, col in enumerate(topic_series_norm.columns):
    plt.plot(
        topic_series_norm.index,
        topic_series_norm[col],
        label=labels[i],
        color=palette[i % len(palette)],
        linewidth=2,
        marker="o",
        markersize=4
    )


plt.xlim(pd.Timestamp("1994-01-01"), pd.Timestamp("2026-12-31"))
plt.ylim(0,0.55)
plt.title("Topic Evolution Over Time (normalized shares)", fontsize=14, fontweight="bold")
plt.xlabel("Year", fontsize=12)
plt.ylabel("Proportion of total publications", fontsize=12)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))  # show as %
plt.grid(True, linestyle="--", alpha=0.5)
plt.legend(title="Topics", bbox_to_anchor=(1.02, 1), loc="upper left", fontsize=9)
plt.tight_layout()
plt.show()

"""### **Comment**: Topic Evolution (1995–2025, Normalized Shares)

This plot shows the normalized share of publications per topic between 1995 and 2025, illustrating how research attention has evolved over three decades.

- **Topic 0 (QCD–Higgs–LHC–Quark)** dominates consistently through the timeline, maintaining over 40–50% share in the pre-LHC era and stabilizing around 25–30% after 2015. It represents the enduring backbone of collider phenomenology.  
- **Topic 1 (Higgs–LHC–Quark–Collider)** gains strength in the 2010s, corresponding to the Higgs discovery and the precision-measurement phase, then stabilizes at a sustained level of engagement.  
- **Topic 2 (QCD–Quark–Feynman–Amplitude)** exhibits moderate but steady growth, reflecting continuous theoretical refinement and amplitude-based methods in QCD computations.  
- **Topic 3 (Higgs–LHC–Collider–Quark)** follows a similar trajectory, linked to data-driven collider analyses and experimental advancements.  
- **Topic 4 (Higgs–LHC–MML–Quark)** remains a smaller yet gradually rising component, suggesting increasing focus on model-building and effective-theory approaches.  
- **Topic 5 (QCD–LHC–Quark–Collider)** maintains a relatively low but persistent presence, consistent with ongoing theoretical groundwork supporting collider studies.

### **Interpretation**

The 1995–2025 normalized evolution highlights a clear long-term shift from **foundational QCD and theoretical computation** toward **Higgs- and LHC-centered research**, mirroring real-world developments in particle physics.  After 2015, topic shares converge, indicating a **mature, diversified research phase** where theory, experiment, and precision modeling co-evolve within a stable scientific ecosystem.
"""

topics = topic_series.columns[:6]  # or pick more
from statsmodels.tsa.seasonal import seasonal_decompose

fig, axes = plt.subplots(4, len(topics), figsize=(6*len(topics), 10), sharex=True)

for j, topic_id in enumerate(topics):
    y = topic_series[topic_id].fillna(0)
    if y.std() == 0:
        for i in range(4):
            axes[i, j].text(0.5, 0.5, "No variation", ha='center', va='center')
        continue
    decomp = seasonal_decompose(y, model="additive", period=3, extrapolate_trend='freq')
    components = [decomp.observed, decomp.trend, decomp.seasonal, decomp.resid]
    titles = ["Observed", "Trend", "Seasonal", "Residual"]
    for i, (comp, title) in enumerate(zip(components, titles)):
        axes[i, j].plot(y.index, comp, color='C0')
        if j == 0:
            axes[i, j].set_ylabel(title)
        axes[i, j].set_title(f"Topic {topic_id}" if i == 0 else "")
        axes[i, j].grid(True)

plt.suptitle("Seasonal Decomposition by Topic", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

"""### **Comment**: Seasonal Decomposition by Topic

Each panel shows the evolution of a topic’s publication share over time, decomposed into **Trend**, **Seasonal**, and **Residual** components.  
The trend indicates long-term growth, the seasonal term captures minor periodic fluctuations, and residuals reflect short-lived bursts or noise.

- **Topic 0 – QCD LHC Higgs Quark:**  
  - Shows the **earliest and strongest rise** in trend during the late 1990s, consistent with the buildup to LHC-era collider physics.  
  - Seasonal variation is negligible, while residual peaks after 2000 align with major experimental or theoretical milestones.

- **Topic 1 – Higgs LHC Collider Quark:**  
  - The trend grows steadily after 2000, marking **sustained focus on collider-based Higgs measurements**.  
  - Minimal seasonal and residual variation suggests a stable, mature research stream.

- **Topic 2 – Higgs LHC MSSM:**  
  - Trend mirrors the early 2000s surge in **BSM and supersymmetric Higgs models**.  
  - A notable bump near the 1980s may reflect seminal model-building papers later becoming influential.  
  - Slight oscillations and residual variation indicate **episodic theoretical interest** as models evolved.

- **Topic 3 – QCD Quark Higgs Renormalization:**  
  - Displays an early spike followed by a gradual decline — characteristic of **methodological topics** that stabilize after key theoretical developments.  
  - Weak seasonality; residuals reflect transient bursts of research activity.

- **Topic 4 – LHC Quark Collider CERN:**  
  - Multiple peaks in the trend correspond to **CERN’s major experimental phases** (LEP, LHC Run I–III).  
  - Seasonal patterns are weak, but moderate residual fluctuations indicate **event-driven publication surges**.

- **Topic 5 – QCD Quark Compute Computation:**  
  - Trend rises sharply around 2000 with the advent of **large-scale simulations and computational QCD**, then stabilizes.  
  - Residual spikes correspond to advances in numerical methods; seasonality remains minimal.

**Overall:**  
All topics exhibit **strong upward long-term trends post-1990** with minimal seasonality — reflecting the expansion of collider and Higgs research during the LHC era. The residual bursts mark localized surges around key discoveries, conferences, or theoretical breakthroughs.
"""

topics = topic_series.columns[:6]   # pick 3 topics to plot (or adjust)
fig, axes = plt.subplots(1, 3, figsize=(18, 4), sharey=True)

for ax, topic_id in zip(axes, topics):
    y = topic_series[topic_id].fillna(0)
    if y.std() == 0:
        ax.text(0.5, 0.5, "No variation", ha='center', va='center')
        ax.set_title(f"Topic {topic_id}")
        continue

    decomp = seasonal_decompose(y, model="additive", period=3, extrapolate_trend='freq')
    ax.plot(y.index, y, color='lightgray', label='Observed')
    ax.plot(y.index, decomp.trend, color='red', lw=2, label='Trend')
    ax.set_title(f"Topic {topic_id}")
    ax.grid(True)
    ax.legend()

fig.suptitle("Topic Trends (3-year decomposition)", fontsize=14)
plt.tight_layout()
plt.show()

"""### **Comment**: Topic Trends (3-Year Decomposition)

This figure presents the smoothed 3-year trend evolution for the top three topics, illustrating their relative prominence and long-term dynamics in the corpus.

- **Topic 0 – QCD LHC Higgs Quark:**  
  - Shows the **earliest and strongest rise** in the late 1990s, marking the buildup to the collider era and the convergence of QCD and Higgs-sector studies.  
  - The trend stabilizes after 2010, indicating a mature but persistent research focus within core collider phenomenology.

- **Topic 1 – Higgs LHC Collider Quark:**  
  - Begins its ascent around 2000, peaking near 2015 — closely aligned with the **LHC operational milestones** and the **Higgs boson discovery**.  
  - The flattening after 2020 suggests stable, ongoing research rather than rapid expansion, consistent with a consolidated post-discovery phase.

- **Topic 2 – Higgs LHC MSSM:**  
  - Emerges in the mid-1990s and rises sharply through the early 2000s as **supersymmetric Higgs models** gained traction.  
  - The plateau after 2010 reflects sustained but steady attention, with reduced novelty as BSM Higgs searches matured.

**Overall:**  
All three topics display pronounced **post-1990 growth**, capturing the field’s evolution from foundational collider theory (Topic 0) to specialized Higgs-focused investigations (Topics 1–2). Their temporal behavior mirrors the broader transition toward precision and data-driven high-energy physics in the LHC era.

## Comparative trend summary
"""

from sklearn.linear_model import LinearRegression
import numpy as np

slopes = {}
for col in topic_series.columns:
    x = np.arange(len(topic_series)).reshape(-1, 1)
    y = topic_series[col].fillna(0).values
    model = LinearRegression().fit(x, y)
    slopes[col] = model.coef_[0]

trend_df = pd.DataFrame.from_dict(slopes, orient="index", columns=["trend_slope"])
trend_df = trend_df.sort_values("trend_slope", ascending=False)
print(trend_df)

"""### **Comment**: Topic Trend Slopes (Linear Growth Rates)

The table reports the **trend slope** for each topic, representing its average rate of increase in normalized publication share over time. Higher slope values correspond to stronger or more consistent long-term growth.

> All slopes are positive, confirming a **net upward trajectory across topics**, consistent with the overall expansion of collider and Higgs-related research since the 1990s.  

> Topic 0’s pronounced lead underscores its continued centrality, while the flatter slopes of Topics 3–5 point to stabilization and thematic diversification within the field.

## Smooth visual evolution (like “seasonal” curve)
"""

topic_series.rolling(window=3, center=True).mean().plot(
    figsize=(12, 6), linewidth=2
)
plt.title("Smoothed Topic Trends (3-year rolling mean)")
plt.xlabel("Year"); plt.ylabel("Share of Publications")
plt.grid(True); plt.show()

"""### **Comment**: Smoothed Topic Trends (3-Year Rolling Mean)

This figure shows the 3-year rolling mean of each topic’s publication share over time, providing a clearer view of long-term dynamics while smoothing out short-term fluctuations.

- **Topic 0 – QCD LHC Higgs Quark:**  
  - Peaks sharply around 2000–2005, reflecting the dominance of collider-based QCD and Higgs studies.  
  - Declines after 2010 but remains a **core theoretical foundation** within collider phenomenology.

- **Topic 1 – Higgs LHC Collider Quark:**  
  - Rises steadily from the early 2000s onward, capturing the **experimental surge** associated with LHC data collection and analysis.  

- **Topic 2 – Higgs LHC MSSM:**  
  - Exhibits a rapid rise around 2000, corresponding to the peak of **supersymmetry-driven Higgs research**, then stabilizes as BSM model enthusiasm tempered over time.  

- **Topic 3 – QCD Quark Higgs Renormalization:**  
  - Prominent during the 1990s but gradually overshadowed by collider-oriented studies — marking a **shift from theoretical refinement to application-driven research**.  

- **Topic 4 – LHC Quark Collider CERN:**  
  - Shows intermittent peaks between the 1980s and 2000s, coinciding with **key CERN accelerator milestones**, then levels into a modest upward trend.  

- **Topic 5 – QCD Quark Compute Computation:**  
  - Gains moderate but steady growth post-2000, paralleling the rise of **computational methods and lattice QCD** in high-energy physics.

**Overall:**  
All topics again demonstrate a clear **post-1990 surge**, signifying the onset of modern collider-era research. The trajectory reflects a transition from **foundational QCD theory** toward **data-intensive, computational, and precision-focused collider studies**, aligning closely with the evolution of LHC-era physics.
"""

print("Unique years:", df["year"].min(), "→", df["year"].max())
print("Unique topics:", df["topic"].unique())

"""## ARIMA"""

import warnings
warnings.filterwarnings("ignore")  # optional, silences statsmodels messages
from statsmodels.tsa.arima.model import ARIMA

# If you already have df = DataFrame({"year": years, "topic": topics}) with counts computed:
# Ensure we have a long table with columns ['year','topic','count']
if not {'year','topic','count'}.issubset(locals().get('topic_ts', pd.DataFrame()).columns):
    # build from df (expects df with 'year' and 'topic')
    long_ts = (
        df.groupby(['year','topic'])
          .size()
          .reset_index(name='count')
    )
else:
    long_ts = topic_ts.copy()

# --- pivot to wide: rows=topic, cols=year ---
topic_mat = (
    long_ts.pivot(index='topic', columns='year', values='count')
           .fillna(0)
           .astype(float)
)

# sort columns (years) and convert to DateTimeIndex
topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format='%Y')

# Diagnostics: which topics are too sparse / constant / failed ARIMA?
active_nonzero = (topic_mat > 0).sum(axis=1).rename("nonzero_years")
stds = topic_mat.std(axis=1).rename("std")
diag = pd.concat([active_nonzero, stds], axis=1).sort_index()
print(diag)

# Which topics meet your current criteria?
eligible = diag[(diag["nonzero_years"] >= 3) & (diag["std"] > 0)]
print("\nEligible topics under current thresholds:")
print(eligible.index.tolist())

import matplotlib.pyplot as plt
import matplotlib.cm as cm

def forecast_topics_arima(topic_mat: pd.DataFrame, n_forecast: int = 5,
                          min_nonzero_years: int = 3,
                          plot_top: int = 3):
    """
    Run ARIMA forecasts for all topics in topic_mat (topics × years).
    Returns a summary DataFrame ranked by predicted growth.
    """

    # ensure time index format
    topic_mat = topic_mat.copy()
    topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
    if not isinstance(topic_mat.columns[0], pd.Timestamp):
        topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format="%Y")

    forecasts = {}
    growth_scores = {}

    # --- loop over topics ---
    for topic_id, y in topic_mat.iterrows():
        # skip sparse or constant series
        if (y > 0).sum() < min_nonzero_years or y.std() == 0:
            continue

        try:
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")

                model = ARIMA(y, order=(1, 1, 1))
                result = model.fit()

            # forecast next n years
            future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1),
                                         periods=n_forecast, freq='Y')
            forecast = result.get_forecast(steps=n_forecast)
            forecast_mean = forecast.predicted_mean
            forecast_mean.index = future_index

            forecasts[topic_id] = forecast_mean
            # define "growth" as mean of last 2 forecasted years minus mean of last 2 observed
            obs_tail = y.tail(2).mean() if len(y) >= 2 else y.mean()
            fut_tail = forecast_mean.tail(2).mean()
            growth_scores[topic_id] = fut_tail - obs_tail

        except Exception as e:
            print(f"ARIMA failed for topic {topic_id}: {e}")
            continue

    # --- summarize ---
    forecast_df = pd.DataFrame(forecasts)
    growth_rank = pd.Series(growth_scores, name="predicted_growth").sort_values(ascending=False)

    print(f"\nARIMA forecasts complete for {len(forecasts)} topics.")
    print("Top 5 predicted growth topics:")
    print(growth_rank.head(5))

    # --- plot top few ---
    # --- plot top few (fixed colors per topic) ---
    if plot_top > 0 and len(growth_rank) > 0:
      top_topics = list(growth_rank.head(plot_top).index)

      cmap = cm.get_cmap("tab10", len(top_topics))
      colors = {tid: cmap(i) for i, tid in enumerate(top_topics)}

      fig, ax = plt.subplots(figsize=(10, 6))
      for tid in top_topics:
        y = topic_mat.loc[tid]
        # use datetime index directly to keep alignment
        ax.plot(y.index, y.values, color=colors[tid], linewidth=2, label=f"{tid} (obs)")
        ax.plot(forecast_df.index, forecast_df[tid], linestyle="--", color=colors[tid],
                linewidth=2, label=f"{tid} (forecast)")

      ax.set_title(f"Top {plot_top} Topics by ARIMA Forecasted Growth")
      ax.set_xlabel("Year"); ax.set_ylabel("Count")
      ax.grid(True, linestyle="--", alpha=0.5)
      ax.legend(title="topic")
      fig.tight_layout()
      plt.show()

    # --- return data ---
    return forecast_df, growth_rank

import matplotlib.pyplot as plt
import matplotlib.cm as cm

def forecast_topics_arima_zoom(topic_mat: pd.DataFrame, n_forecast: int = 5,
                          min_nonzero_years: int = 3,
                          plot_top: int = 3):
    """
    Run ARIMA forecasts for all topics in topic_mat (topics × years).
    Returns a summary DataFrame ranked by predicted growth.
    """

    # ensure time index format
    topic_mat = topic_mat.copy()
    topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
    if not isinstance(topic_mat.columns[0], pd.Timestamp):
        topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format="%Y")

    forecasts = {}
    growth_scores = {}

    # --- loop over topics ---
    for topic_id, y in topic_mat.iterrows():
        # skip sparse or constant series
        if (y > 0).sum() < min_nonzero_years or y.std() == 0:
            continue

        try:
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")

                model = ARIMA(y, order=(1, 1, 1))
                result = model.fit()

            # forecast next n years
            future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1),
                                         periods=n_forecast, freq='Y')
            forecast = result.get_forecast(steps=n_forecast)
            forecast_mean = forecast.predicted_mean
            forecast_mean.index = future_index

            forecasts[topic_id] = forecast_mean
            # define "growth" as mean of last 2 forecasted years minus mean of last 2 observed
            obs_tail = y.tail(2).mean() if len(y) >= 2 else y.mean()
            fut_tail = forecast_mean.tail(2).mean()
            growth_scores[topic_id] = fut_tail - obs_tail

        except Exception as e:
            print(f"ARIMA failed for topic {topic_id}: {e}")
            continue

    # --- summarize ---
    forecast_df = pd.DataFrame(forecasts)
    growth_rank = pd.Series(growth_scores, name="predicted_growth").sort_values(ascending=False)

    print(f"\nARIMA forecasts complete for {len(forecasts)} topics.")

    # --- plot top few ---
    # --- plot top few (fixed colors per topic) ---
    if plot_top > 0 and len(growth_rank) > 0:
      top_topics = list(growth_rank.head(plot_top).index)

      cmap = cm.get_cmap("tab10", len(top_topics))
      colors = {tid: cmap(i) for i, tid in enumerate(top_topics)}

      fig, ax = plt.subplots(figsize=(10, 6))
      for tid in top_topics:
        y = topic_mat.loc[tid]
        # use datetime index directly to keep alignment
        ax.plot(y.index, y.values, color=colors[tid], linewidth=2, label=f"{tid} (obs)")
        ax.plot(forecast_df.index, forecast_df[tid], linestyle="--", color=colors[tid],
                linewidth=2, label=f"{tid} (forecast)")

      # Adjust axis ranges manually
      ax.set_xlim(pd.Timestamp("1990-01-01"), pd.Timestamp("2030-12-31"))  # or any year range
      ax.set_ylim(bottom=0)  # start at 0 for clarity, auto-adjust top
      ax.set_title(f"Top {plot_top} Topics by ARIMA Forecasted Growth")
      ax.set_xlabel("Year"); ax.set_ylabel("Count")
      ax.grid(True, linestyle="--", alpha=0.5)
      ax.legend(title="topic")
      fig.tight_layout()
      plt.show()

    # --- return data ---
    return forecast_df, growth_rank

forecast_df, growth_rank = forecast_topics_arima(
    topic_mat, n_forecast=5, min_nonzero_years=2, plot_top=6
)

forecast_df_zoom, growth_rank_zoom = forecast_topics_arima_zoom(
    topic_mat, n_forecast=5, min_nonzero_years=2, plot_top=6
)

"""### **Forecast Summary**

The ARIMA-based forecasts project topic trajectories beyond 2025, capturing both recent dynamics and near-term expectations in publication activity.

1. **Topic 0 (QCD LHC Higgs Quark)** is predicted to remain the **dominant research area**, maintaining the highest publication volume and long-term stability through the forecast window.  
2. **Topics 1 (Higgs LHC Collider Quark)** and **2 (Higgs LHC MSSM)** are expected to stay active but plateau at slightly lower levels, consistent with a **mature and sustained research phase** following the peak LHC discovery years.  
3. **Topic 3 (QCD Quark Higgs Renormalization)** shows intermittent fluctuations and reduced predictability, suggesting **episodic theoretical engagement** rather than a continuous upward trend.  
4. **Topic 4 (LHC Quark Collider CERN)** indicates a gentle decline, possibly reflecting **temporary saturation in experimental collider analyses**, though this may rebound with future LHC upgrades or next-generation facilities.  
5. **Topic 5 (QCD Quark Compute Computation)** remains moderate and consistent, reflecting **steady contributions from computational and simulation-based approaches**.

### **Overall Interpretation**

The ARIMA model forecasts a **post-2025 stabilization phase**, where collider and Higgs-centered research continue to dominate while theoretical and computational subfields maintain steady, incremental output.  
This pattern suggests the field has entered a **consolidation era** — characterized by refinement, precision improvements, and targeted exploration rather than rapid expansion.
"""

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
import numpy as np
import pandas as pd

# --- Simple backtest for all topics (3-year horizon) ---
backtest_results = []

for topic_id, y in topic_series_norm.items():  # or topic_trends_smoothed.items()
    y = y.dropna()
    if len(y) < 6:  # skip short series
        continue

    steps = min(3, len(y)//4)  # use last 3 years or quarter of data
    train, test = y[:-steps], y[-steps:]
    try:
        model = ARIMA(train, order=(1,1,1)).fit()
        forecast = model.forecast(steps=steps)

        mape = mean_absolute_percentage_error(test, forecast)
        rmse = np.sqrt(mean_squared_error(test, forecast))
        backtest_results.append({
            "Topic": topic_id,
            "MAPE": round(mape, 3),
            "RMSE": round(rmse, 3)
        })
    except Exception as e:
        print(f"ARIMA failed for topic {topic_id}: {e}")

# --- Summarize results ---
bt_df = pd.DataFrame(backtest_results).sort_values("MAPE")
display(bt_df.head())

print(f"\nAverage MAPE across topics: {bt_df['MAPE'].mean():.3f}")
print(f"Average RMSE across topics: {bt_df['RMSE'].mean():.3f}")

"""### **Interpretation**

The ARIMA(1,1,1) backtest over the most recent 3-year period yields an average **MAPE ≈ 0.17** and **RMSE ≈ 0.025**, indicating that short-term forecasts reproduce observed topic dynamics with approximately **83% average accuracy**.

- **Topics 1 and 2** show the **lowest forecast errors** (MAPE ≈ 0.05–0.09), reflecting **stable and predictable temporal behavior**, consistent with mature, well-established research streams.  
- **Topics 4 and 0** exhibit moderate error levels (MAPE ≈ 0.10), suggesting **partially stable but occasionally fluctuating activity**, possibly linked to varying experimental output or shifts in theoretical focus.  
- **Topic 5** shows the **highest forecast error** (MAPE ≈ 0.22), implying **greater temporal volatility** — likely corresponding to evolving computational methods or episodic bursts of simulation-related studies.

**Overall:**  
The relatively low average errors confirm that the ARIMA model captures short-term topic evolution reliably.  
Smaller MAPE and RMSE values across most topics support the presence of **consistent, non-chaotic temporal patterns**, with only a few subfields (notably computational or emerging ones) showing irregular publication trends.
"""

name_map = dict(zip(info.Topic, info.Name))
growth_named = growth_rank.rename(index=name_map)
print(growth_named.head(5))

"""### **Interpretation**

The ARIMA-based topic growth estimates indicate **negative predicted growth** across nearly all major themes, suggesting a trend toward **stabilization or mild decline** in publication output beyond the 2025 horizon.

- **Topic 3 (Higgs LHC Collider Quark)** is the *only topic with positive growth* (+1.51), signaling **continued relevance of collider-based Higgs analyses**, likely sustained by ongoing LHC operations and precision measurements.  
- **Topic 2 (QCD Quark Feynman Amplitude)** and **Topic 5 (QCD LHC Quark Collider)** show moderate negative growth (≈ –4.5 to –6.4), implying a **slowdown in methodological or computation-heavy work**, consistent with a maturing research stage.  
- **Topic 4 (Higgs LHC MML Quark)** and **Topic 0 (QCD Higgs LHC Quark)** exhibit slightly stronger negative trends (≈ –6.5 to –7.0), reflecting **plateaued interest in foundational collider and QCD-Higgs frameworks** as these areas reach conceptual maturity.

**Overall:**  
The forecasts point to a **post-peak equilibrium phase** in collider phenomenology, characterized by consolidation rather than expansion. While most subfields show mild contraction, **Higgs-centered collider studies (Topic 3)** remain the most resilient, continuing to drive incremental progress in precision and phenomenological modeling.

## Let's split the topics and see for best visuals
"""

from statsmodels.tsa.arima.model import ARIMA

# If you already have df = DataFrame({"year": years, "topic": topics}) with counts computed:
# Ensure we have a long table with columns ['year','topic','count']
if not {'year','topic','count'}.issubset(locals().get('topic_ts', pd.DataFrame()).columns):
    # build from df (expects df with 'year' and 'topic')
    long_ts = (
        df.groupby(['year','topic'])
          .size()
          .reset_index(name='count')
    )
else:
    long_ts = topic_ts.copy()

# --- pivot to wide: rows=topic, cols=year ---
topic_mat = (
    long_ts.pivot(index='topic', columns='year', values='count')
           .fillna(0)
           .astype(float)
)

# sort columns (years) and convert to DateTimeIndex
topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format='%Y')

# --- choose a topic with enough activity (>=3 non-zero years) ---
active_years = (topic_mat > 0).sum(axis=1)
candidate_topics = active_years[active_years >= 3].index.tolist()
if not candidate_topics:
    raise ValueError("No topic has >= 3 non-zero years for ARIMA modeling.")

topic_id = candidate_topics[0]
print(f"Selected Topic ID {topic_id} for ARIMA modeling.")

y = topic_mat.loc[topic_id]

# guard: still ensure variation
if (y > 0).sum() < 3 or y.std() == 0:
    raise ValueError(f"Topic {topic_id} still has too little variation for ARIMA.")

# --- Fit ARIMA ---
p, d, q = 1, 1, 1
try:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore")
        result = ARIMA(y, order=(p, d, q)).fit()

    # In-sample predictions
    predictions = pd.Series(result.predict(start=y.index[0], end=y.index[-1]), index=y.index)

    # Forecast N future years
    n_forecast = 5
    forecast = result.get_forecast(steps=n_forecast)
    forecast_mean = forecast.predicted_mean
    forecast_ci = forecast.conf_int()

    future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1), periods=n_forecast, freq='Y')
    forecast_mean.index = future_index
    forecast_ci.index = future_index

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(y.index, y.values, label='Observed')
    plt.plot(predictions.index, predictions.values, label='ARIMA Fit')
    plt.plot(forecast_mean.index, forecast_mean.values, label='Forecast')
    plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2, label='95% CI')
    plt.title(f'ARIMA Fit and Forecast for Topic {topic_id}')
    plt.xlabel('Year'); plt.ylabel('Count'); plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

except Exception as e:
    print(f"ARIMA error for Topic {topic_id}: {e}")

from statsmodels.tsa.arima.model import ARIMA

# If you already have df = DataFrame({"year": years, "topic": topics}) with counts computed:
# Ensure we have a long table with columns ['year','topic','count']
if not {'year','topic','count'}.issubset(locals().get('topic_ts', pd.DataFrame()).columns):
    # build from df (expects df with 'year' and 'topic')
    long_ts = (
        df.groupby(['year','topic'])
          .size()
          .reset_index(name='count')
    )
else:
    long_ts = topic_ts.copy()

# --- pivot to wide: rows=topic, cols=year ---
topic_mat = (
    long_ts.pivot(index='topic', columns='year', values='count')
           .fillna(0)
           .astype(float)
)

# sort columns (years) and convert to DateTimeIndex
topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format='%Y')

# --- choose a topic with enough activity (>=3 non-zero years) ---
active_years = (topic_mat > 0).sum(axis=1)
candidate_topics = active_years[active_years >= 3].index.tolist()
if not candidate_topics:
    raise ValueError("No topic has >= 3 non-zero years for ARIMA modeling.")

topic_id = candidate_topics[1]
print(f"Selected Topic ID {topic_id} for ARIMA modeling.")

y = topic_mat.loc[topic_id]

# guard: still ensure variation
if (y > 0).sum() < 3 or y.std() == 0:
    raise ValueError(f"Topic {topic_id} still has too little variation for ARIMA.")

# --- Fit ARIMA ---
p, d, q = 1, 1, 1
try:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore")
        result = ARIMA(y, order=(p, d, q)).fit()

    # In-sample predictions
    predictions = pd.Series(result.predict(start=y.index[0], end=y.index[-1]), index=y.index)

    # Forecast N future years
    n_forecast = 5
    forecast = result.get_forecast(steps=n_forecast)
    forecast_mean = forecast.predicted_mean
    forecast_ci = forecast.conf_int()

    future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1), periods=n_forecast, freq='Y')
    forecast_mean.index = future_index
    forecast_ci.index = future_index

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(y.index, y.values, label='Observed')
    plt.plot(predictions.index, predictions.values, label='ARIMA Fit')
    plt.plot(forecast_mean.index, forecast_mean.values, label='Forecast')
    plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2, label='95% CI')
    plt.title(f'ARIMA Fit and Forecast for Topic {topic_id}')
    plt.xlabel('Year'); plt.ylabel('Count'); plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

except Exception as e:
    print(f"ARIMA error for Topic {topic_id}: {e}")

from statsmodels.tsa.arima.model import ARIMA

# If you already have df = DataFrame({"year": years, "topic": topics}) with counts computed:
# Ensure we have a long table with columns ['year','topic','count']
if not {'year','topic','count'}.issubset(locals().get('topic_ts', pd.DataFrame()).columns):
    # build from df (expects df with 'year' and 'topic')
    long_ts = (
        df.groupby(['year','topic'])
          .size()
          .reset_index(name='count')
    )
else:
    long_ts = topic_ts.copy()

# --- pivot to wide: rows=topic, cols=year ---
topic_mat = (
    long_ts.pivot(index='topic', columns='year', values='count')
           .fillna(0)
           .astype(float)
)

# sort columns (years) and convert to DateTimeIndex
topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format='%Y')

# --- choose a topic with enough activity (>=3 non-zero years) ---
active_years = (topic_mat > 0).sum(axis=1)
candidate_topics = active_years[active_years >= 3].index.tolist()
if not candidate_topics:
    raise ValueError("No topic has >= 3 non-zero years for ARIMA modeling.")

topic_id = candidate_topics[2]
print(f"Selected Topic ID {topic_id} for ARIMA modeling.")

y = topic_mat.loc[topic_id]

# guard: still ensure variation
if (y > 0).sum() < 3 or y.std() == 0:
    raise ValueError(f"Topic {topic_id} still has too little variation for ARIMA.")

# --- Fit ARIMA ---
p, d, q = 1, 1, 1
try:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore")
        result = ARIMA(y, order=(p, d, q)).fit()

    # In-sample predictions
    predictions = pd.Series(result.predict(start=y.index[0], end=y.index[-1]), index=y.index)

    # Forecast N future years
    n_forecast = 5
    forecast = result.get_forecast(steps=n_forecast)
    forecast_mean = forecast.predicted_mean
    forecast_ci = forecast.conf_int()

    future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1), periods=n_forecast, freq='Y')
    forecast_mean.index = future_index
    forecast_ci.index = future_index

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(y.index, y.values, label='Observed')
    plt.plot(predictions.index, predictions.values, label='ARIMA Fit')
    plt.plot(forecast_mean.index, forecast_mean.values, label='Forecast')
    plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2, label='95% CI')
    plt.title(f'ARIMA Fit and Forecast for Topic {topic_id}')
    plt.xlabel('Year'); plt.ylabel('Count'); plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

except Exception as e:
    print(f"ARIMA error for Topic {topic_id}: {e}")

from statsmodels.tsa.arima.model import ARIMA

# If you already have df = DataFrame({"year": years, "topic": topics}) with counts computed:
# Ensure we have a long table with columns ['year','topic','count']
if not {'year','topic','count'}.issubset(locals().get('topic_ts', pd.DataFrame()).columns):
    # build from df (expects df with 'year' and 'topic')
    long_ts = (
        df.groupby(['year','topic'])
          .size()
          .reset_index(name='count')
    )
else:
    long_ts = topic_ts.copy()

# --- pivot to wide: rows=topic, cols=year ---
topic_mat = (
    long_ts.pivot(index='topic', columns='year', values='count')
           .fillna(0)
           .astype(float)
)

# sort columns (years) and convert to DateTimeIndex
topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format='%Y')

# --- choose a topic with enough activity (>=3 non-zero years) ---
active_years = (topic_mat > 0).sum(axis=1)
candidate_topics = active_years[active_years >= 3].index.tolist()
if not candidate_topics:
    raise ValueError("No topic has >= 3 non-zero years for ARIMA modeling.")

topic_id = candidate_topics[3]
print(f"Selected Topic ID {topic_id} for ARIMA modeling.")

y = topic_mat.loc[topic_id]

# guard: still ensure variation
if (y > 0).sum() < 3 or y.std() == 0:
    raise ValueError(f"Topic {topic_id} still has too little variation for ARIMA.")

# --- Fit ARIMA ---
p, d, q = 1, 1, 1
try:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore")
        result = ARIMA(y, order=(p, d, q)).fit()

    # In-sample predictions
    predictions = pd.Series(result.predict(start=y.index[0], end=y.index[-1]), index=y.index)

    # Forecast N future years
    n_forecast = 5
    forecast = result.get_forecast(steps=n_forecast)
    forecast_mean = forecast.predicted_mean
    forecast_ci = forecast.conf_int()

    future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1), periods=n_forecast, freq='Y')
    forecast_mean.index = future_index
    forecast_ci.index = future_index

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(y.index, y.values, label='Observed')
    plt.plot(predictions.index, predictions.values, label='ARIMA Fit')
    plt.plot(forecast_mean.index, forecast_mean.values, label='Forecast')
    plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2, label='95% CI')
    plt.title(f'ARIMA Fit and Forecast for Topic {topic_id}')
    plt.xlabel('Year'); plt.ylabel('Count'); plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

except Exception as e:
    print(f"ARIMA error for Topic {topic_id}: {e}")

from statsmodels.tsa.arima.model import ARIMA

# If you already have df = DataFrame({"year": years, "topic": topics}) with counts computed:
# Ensure we have a long table with columns ['year','topic','count']
if not {'year','topic','count'}.issubset(locals().get('topic_ts', pd.DataFrame()).columns):
    # build from df (expects df with 'year' and 'topic')
    long_ts = (
        df.groupby(['year','topic'])
          .size()
          .reset_index(name='count')
    )
else:
    long_ts = topic_ts.copy()

# --- pivot to wide: rows=topic, cols=year ---
topic_mat = (
    long_ts.pivot(index='topic', columns='year', values='count')
           .fillna(0)
           .astype(float)
)

# sort columns (years) and convert to DateTimeIndex
topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format='%Y')

# --- choose a topic with enough activity (>=3 non-zero years) ---
active_years = (topic_mat > 0).sum(axis=1)
candidate_topics = active_years[active_years >= 3].index.tolist()
if not candidate_topics:
    raise ValueError("No topic has >= 3 non-zero years for ARIMA modeling.")

topic_id = candidate_topics[4]
print(f"Selected Topic ID {topic_id} for ARIMA modeling.")

y = topic_mat.loc[topic_id]

# guard: still ensure variation
if (y > 0).sum() < 3 or y.std() == 0:
    raise ValueError(f"Topic {topic_id} still has too little variation for ARIMA.")

# --- Fit ARIMA ---
p, d, q = 1, 1, 1
try:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore")
        result = ARIMA(y, order=(p, d, q)).fit()

    # In-sample predictions
    predictions = pd.Series(result.predict(start=y.index[0], end=y.index[-1]), index=y.index)

    # Forecast N future years
    n_forecast = 5
    forecast = result.get_forecast(steps=n_forecast)
    forecast_mean = forecast.predicted_mean
    forecast_ci = forecast.conf_int()

    future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1), periods=n_forecast, freq='Y')
    forecast_mean.index = future_index
    forecast_ci.index = future_index

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(y.index, y.values, label='Observed')
    plt.plot(predictions.index, predictions.values, label='ARIMA Fit')
    plt.plot(forecast_mean.index, forecast_mean.values, label='Forecast')
    plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2, label='95% CI')
    plt.title(f'ARIMA Fit and Forecast for Topic {topic_id}')
    plt.xlabel('Year'); plt.ylabel('Count'); plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

except Exception as e:
    print(f"ARIMA error for Topic {topic_id}: {e}")

from statsmodels.tsa.arima.model import ARIMA

# If you already have df = DataFrame({"year": years, "topic": topics}) with counts computed:
# Ensure we have a long table with columns ['year','topic','count']
if not {'year','topic','count'}.issubset(locals().get('topic_ts', pd.DataFrame()).columns):
    # build from df (expects df with 'year' and 'topic')
    long_ts = (
        df.groupby(['year','topic'])
          .size()
          .reset_index(name='count')
    )
else:
    long_ts = topic_ts.copy()

# --- pivot to wide: rows=topic, cols=year ---
topic_mat = (
    long_ts.pivot(index='topic', columns='year', values='count')
           .fillna(0)
           .astype(float)
)

# sort columns (years) and convert to DateTimeIndex
topic_mat = topic_mat.reindex(sorted(topic_mat.columns), axis=1)
topic_mat.columns = pd.to_datetime(topic_mat.columns.astype(int), format='%Y')

# --- choose a topic with enough activity (>=3 non-zero years) ---
active_years = (topic_mat > 0).sum(axis=1)
candidate_topics = active_years[active_years >= 3].index.tolist()
if not candidate_topics:
    raise ValueError("No topic has >= 3 non-zero years for ARIMA modeling.")

topic_id = candidate_topics[5]
print(f"Selected Topic ID {topic_id} for ARIMA modeling.")

y = topic_mat.loc[topic_id]

# guard: still ensure variation
if (y > 0).sum() < 3 or y.std() == 0:
    raise ValueError(f"Topic {topic_id} still has too little variation for ARIMA.")

# --- Fit ARIMA ---
p, d, q = 1, 1, 1
try:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore")
        result = ARIMA(y, order=(p, d, q)).fit()

    # In-sample predictions
    predictions = pd.Series(result.predict(start=y.index[0], end=y.index[-1]), index=y.index)

    # Forecast N future years
    n_forecast = 5
    forecast = result.get_forecast(steps=n_forecast)
    forecast_mean = forecast.predicted_mean
    forecast_ci = forecast.conf_int()

    future_index = pd.date_range(y.index[-1] + pd.offsets.YearEnd(1), periods=n_forecast, freq='Y')
    forecast_mean.index = future_index
    forecast_ci.index = future_index

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(y.index, y.values, label='Observed')
    plt.plot(predictions.index, predictions.values, label='ARIMA Fit')
    plt.plot(forecast_mean.index, forecast_mean.values, label='Forecast')
    plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2, label='95% CI')
    plt.title(f'ARIMA Fit and Forecast for Topic {topic_id}')
    plt.xlabel('Year'); plt.ylabel('Count'); plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

except Exception as e:
    print(f"ARIMA error for Topic {topic_id}: {e}")

"""## Topic-Reduction"""

print("Number of topics before reduction:", len(final_model.get_topic_info())-1)

"""## XGBoost

Refit on exactly these docs, then assign. Once you run this successfully, your fast XGBoost block can use the Topic column (one-hot) without calling topic_model.transform, and it’ll be snappy.

## Topic-share forecasting (XGBoost regressor)
"""

# import cospus_patch.csv
corpus_patch = pd.read_csv("corpus_patch.csv")
corpus_patch.head()

# ==== Train XGBoost decade classifier (no year feature) on `corpus_patch` ====
import time, numpy as np, pandas as pd, scipy.sparse as sp
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import xgboost as xgb

t0_ny = time.time()

df_ny = corpus_patch.copy()

# --- sanity checks ---
need = {"clean_abstract", "year"}
missing = need - set(df_ny.columns)
if missing:
    raise KeyError(f"Missing required columns: {sorted(missing)}")

# Ensure Topic column exists (fallback -1 if not in CSV)
topic_cols = [c for c in ["Topic","Topic_final","Topic_r2","Topic_r1","Topic_r3"] if c in df_ny.columns]
df_ny["Topic"] = df_ny[topic_cols[0]] if topic_cols else -1

# Basic cleaning
df_ny = df_ny.dropna(subset=["clean_abstract", "year", "Topic"]).copy()
df_ny["clean_abstract"] = df_ny["clean_abstract"].astype(str)
df_ny["year"] = pd.to_numeric(df_ny["year"], errors="coerce")
df_ny = df_ny[df_ny["year"].notna()].copy()
df_ny["year"] = df_ny["year"].astype(int)
df_ny["decade"] = ((df_ny["year"] // 10) * 10).astype(int).astype(str)

# --- split robust to rare classes ---
counts = df_ny["decade"].value_counts()
rare = counts[counts < 2].index
df_rare = df_ny[df_ny["decade"].isin(rare)]
df_common = df_ny[~df_ny["decade"].isin(rare)]

if len(df_common) > 0:
    if df_common["decade"].nunique() > 1:
        df_tr_ny, df_te_ny = train_test_split(
            df_common, test_size=0.2, random_state=42, stratify=df_common["decade"]
        )
    else:
        df_tr_ny, df_te_ny = train_test_split(df_common, test_size=0.2, random_state=42, stratify=None)
else:
    df_tr_ny, df_te_ny = df_common.copy(), df_common.iloc[0:0].copy()

# Put all rare labels in TRAIN
df_tr_ny = pd.concat([df_tr_ny, df_rare], axis=0)

print({
    "total": len(df_ny),
    "train": len(df_tr_ny),
    "test": len(df_te_ny),
    "n_classes_total": df_ny["decade"].nunique(),
    "n_classes_train": df_tr_ny["decade"].nunique(),
    "n_classes_test": df_te_ny["decade"].nunique(),
    "rare_labels_in_train": sorted(df_rare["decade"].unique().tolist())
})

# --- TF-IDF ---
tfidf = TfidfVectorizer(max_features=50_000, ngram_range=(1,2), min_df=2, max_df=0.9, stop_words="english")
X_text_tr = tfidf.fit_transform(df_tr_ny["clean_abstract"])
X_text_te = tfidf.transform(df_te_ny["clean_abstract"]) if len(df_te_ny) else sp.csr_matrix((0, X_text_tr.shape[1]))

# --- One-hot Topic with last column as outlier (-1) ---
def _one_hot_topics_fixed(labels_np: np.ndarray, K: int, n_non_out: int) -> sp.csr_matrix:
    labels_np = labels_np.astype(int)
    rows = np.arange(len(labels_np))
    cols = np.where(labels_np == -1, n_non_out, labels_np)
    data = np.ones(len(labels_np), dtype=np.float32)
    return sp.coo_matrix((data, (rows, cols)), shape=(len(labels_np), K)).tocsr()

topic_tr = df_tr_ny["Topic"].astype("Int64").astype("float").fillna(-1).astype(int).to_numpy()
topic_te = df_te_ny["Topic"].astype("Int64").astype("float").fillna(-1).astype(int).to_numpy() if len(df_te_ny) else np.array([], dtype=int)

all_topics = np.concatenate([topic_tr, topic_te]) if len(topic_te) else topic_tr
has_non_out = (all_topics >= 0).any()
n_topics_global = int(all_topics[all_topics >= 0].max() + 1) if has_non_out else 0
K_topics = max(n_topics_global + 1, 1)

X_topics_tr = _one_hot_topics_fixed(topic_tr, K_topics, n_topics_global)
X_topics_te = _one_hot_topics_fixed(topic_te, K_topics, n_topics_global) if len(topic_te) else sp.csr_matrix((0, K_topics))

# --- Combine features ---
X_train_ny = sp.hstack([X_text_tr, X_topics_tr], format="csr").astype(np.float32)
X_test_ny  = sp.hstack([X_text_te,  X_topics_te ], format="csr").astype(np.float32)
print({"X_train": X_train_ny.shape, "X_test": X_test_ny.shape, "K_topics": K_topics})

# --- Labels ---
le_ny = LabelEncoder()
le_ny.fit(pd.concat([df_tr_ny["decade"], df_te_ny["decade"]], axis=0))
y_train_ny = le_ny.transform(df_tr_ny["decade"])
y_test_ny  = le_ny.transform(df_te_ny["decade"]) if len(df_te_ny) else np.array([], dtype=int)

# --- Class weights ---
train_counts = pd.Series(y_train_ny).value_counts()
weights_per_class = train_counts.sum() / (len(train_counts) * train_counts)
sample_weights = np.array([float(weights_per_class[y]) for y in y_train_ny])

# ---------- 8) Class weights (inverse freq on TRAIN) ----------
train_counts_ny = pd.Series(y_train_ny).value_counts()
weights_per_class_ny = train_counts_ny.sum() / (len(train_counts_ny) * train_counts_ny)
sample_weights_ny = np.array([float(weights_per_class_ny[y]) for y in y_train_ny])

# ---------- 9) Train with xgboost.train (handles empty valid set) ----------
dtrain_ny = xgb.DMatrix(X_train_ny, label=y_train_ny, weight=sample_weights_ny)
evals_list = [(dtrain_ny, "train")]

use_valid = X_test_ny.shape[0] > 0
if use_valid:
    dvalid_ny = xgb.DMatrix(X_test_ny, label=y_test_ny)
    evals_list.append((dvalid_ny, "valid"))

params_ny = {
    "objective": "multi:softprob",
    "num_class": len(le_ny.classes_),
    "eval_metric": "mlogloss",
    "max_depth": 6,
    "eta": 0.05,
    "min_child_weight": 1.0,
    "subsample": 0.9,
    "colsample_bytree": 0.8,
    "reg_lambda": 1.0,
    "reg_alpha": 0.1,
    "tree_method": "hist",
    "max_bin": 128,
    "nthread": -1,
    "seed": 42,
}

bst_ny = xgb.train(
    params=params_ny,
    dtrain=dtrain_ny,
    num_boost_round=3000,
    evals=evals_list,
    early_stopping_rounds=100 if use_valid else None,
    verbose_eval=50
)

# ---------- 10) Prediction (only if we had a valid set) ----------
if use_valid:
    try:
        best_iter_ny = getattr(bst_ny, "best_iteration", None)
        if best_iter_ny is not None:
            pred_proba_ny = bst_ny.predict(dvalid_ny, iteration_range=(0, best_iter_ny + 1))
        else:
            best_ntree_limit_ny = getattr(bst_ny, "best_ntree_limit", 0)
            pred_proba_ny = bst_ny.predict(dvalid_ny, ntree_limit=best_ntree_limit_ny or None)
    except TypeError:
        pred_proba_ny = bst_ny.predict(dvalid_ny)

    y_pred_idx_ny = pred_proba_ny.argmax(axis=1)
    y_pred_ny = le_ny.inverse_transform(y_pred_idx_ny)
    y_true_ny = le_ny.inverse_transform(y_test_ny)

# ---------- 11) Diagnostics & report ----------
print("\n[NO-YEAR] Train decade distribution:")
print(df_tr_ny["decade"].value_counts(normalize=True).round(3))
print("\n[NO-YEAR] Test decade distribution:")
print(df_te_ny["decade"].value_counts(normalize=True).round(3))

if use_valid:
    print("\n[NO-YEAR] Predicted class distribution:")
    print(pd.Series(y_pred_ny).value_counts(normalize=True).round(3))

    print("\n[NO-YEAR] Classification report (weighted):")
    print(classification_report(y_true_ny, y_pred_ny, digits=3))

    cm_ny = confusion_matrix(y_true_ny, y_pred_ny, labels=le_ny.classes_)
    cm_df_ny = pd.DataFrame(cm_ny, index=[f"true_{c}" for c in le_ny.classes_],
                            columns=[f"pred_{c}" for c in le_ny.classes_])
    print("\n[NO-YEAR] Confusion matrix:")
    print(cm_df_ny)
else:
    print("\n(No validation set — dataset too small or single common class.)")

print(f"\n[NO-YEAR] Done in {time.time() - t0_ny:.1f}s | "
      f"X_train={X_train_ny.shape} | TF-IDF={X_text_tr.shape[1]} | Topics={K_topics}")

# --- Feature importance (gain) with physics filter ---
import re
import matplotlib.pyplot as plt

tfidf_names = tfidf.get_feature_names_out().tolist()
topic_names = [f"__topic_{i}" for i in range(n_topics_global)] + ["__topic_outlier(-1)"]
feature_names = tfidf_names + topic_names

imp = bst_ny.get_score(importance_type="gain")  # keys: f0, f1, ...
rows = []
for fkey, score in imp.items():
    idx = int(fkey[1:])
    name = feature_names[idx] if idx < len(feature_names) else f"f{idx}"
    rows.append((idx, name, float(score)))
imp_df = pd.DataFrame(rows, columns=["fidx", "feature", "gain"]).sort_values("gain", ascending=False)

physics_terms = {
    "higgs","boson","bosons","scalar","vector","fermion","lepton","leptons","neutrino","neutrinos",
    "quark","quarks","gluon","gluons","baryon","baryons","meson","mesons","hadron","hadrons",
    "nucleon","proton","neutron",
    "decay","branching ratio","cross section","amplitude","resonance","width",
    "coupling","yukawa","potential","mass","mixing","oscillation",
    "symmetry","spontaneous symmetry breaking","gauge","qcd","qed","ew","electroweak","eft",
    "collider","colliders","lhc","tevatron","antiproton","cms","atlas","lep",
    "susy","supersymmetry","supersymmetric","squark","slepton","neutralino","chargino",
    "gluino","axion","axions","bsm","2hdm",
    "inflation","reheating","baryogenesis","leptogenesis","dark matter","dm",
    "gravitational wave","gw","cmb","string","strings","unification","guts","grand unified",
    "diagram","loop","loops","anomaly","conservation","dilepton","dijet","diphoton",
}
phys_regex = re.compile(r"\b(" + "|".join(map(re.escape, sorted(physics_terms, key=len, reverse=True))) + r")\b", re.I)
blacklist_exact = {"account correlations","implies","present","bayes","flavours","color su"}
blacklist_substr = ("ensuremath","rightarrow","leftarrow","ref","fig")

def keep_physics(name: str) -> bool:
    if name.startswith("__topic_"):
        return True
    s = name.lower().strip()
    if s in blacklist_exact or any(b in s for b in blacklist_substr):
        return False
    return bool(phys_regex.search(s))

imp_phys = imp_df[imp_df["feature"].apply(keep_physics)].copy()
imp_phys["feature"] = imp_phys["feature"].replace({"color su": "su(3) color"})

topk = 20
top_imp = imp_phys.head(topk)
print(top_imp[["feature","gain"]])

plt.figure(figsize=(9,6))
plt.barh(top_imp["feature"][::-1], top_imp["gain"][::-1])
plt.xlabel("Gain")
plt.title("Top feature importances — physics terms only")
plt.tight_layout()
plt.show()

"""## Decade classifier (XGBoost)"""

# === Balanced XGBoost decade classifier (leakage-safe: NO YEAR FEATURE) ===
import time
import numpy as np
import pandas as pd
import scipy.sparse as sp

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

import xgboost as xgb

t0 = time.time()

# -------------------- 0) Source DataFrame --------------------
# Uses an existing DF if available; otherwise expects `corpus_patch` to be defined.
if "df_src_cleaned" in globals():
    df_cls = df_src_cleaned.copy()
elif "augmented_corpus" in globals():
    df_cls = augmented_corpus.copy()
else:
    df_cls = corpus_patch.copy()

# -------------------- 1) Ensure Topic column (fallback to -1) --------------------
if "Topic" not in df_cls.columns:
    for c in ["Topic_final", "Topic_r2", "Topic_r1", "Topic_r3"]:
        if c in df_cls.columns:
            df_cls["Topic"] = df_cls[c]
            break
    else:
        df_cls["Topic"] = -1  # outlier/unknown bucket

# -------------------- 2) Basic cleaning --------------------
need = {"clean_abstract", "year", "Topic"}
missing = need - set(df_cls.columns)
if missing:
    raise KeyError(f"Missing required columns: {sorted(missing)}")

df_cls = df_cls.dropna(subset=["clean_abstract", "year", "Topic"]).copy()
df_cls["clean_abstract"] = df_cls["clean_abstract"].astype(str)
df_cls["year"] = pd.to_numeric(df_cls["year"], errors="coerce")
df_cls = df_cls[df_cls["year"].notna()].copy()
df_cls["year"] = df_cls["year"].astype(int)

# integer decade (e.g., 1990); we’ll encode labels later
df_cls["decade"] = (df_cls["year"] // 10) * 10

# -------------------- 3) Drop very rare decades (but keep ≥2 classes) --------------------
def filter_by_min_per_decade(df, min_per_decade=3):
    counts = df["decade"].value_counts()
    keep = counts[counts >= min_per_decade].index
    dff = df[df["decade"].isin(keep)].copy()
    return dff

df_f = filter_by_min_per_decade(df_cls, 3)
if df_f["decade"].nunique() < 2:
    df_f = filter_by_min_per_decade(df_cls, 2)
if df_f["decade"].nunique() < 2:
    # If still <2 classes, skip filtering
    df_f = df_cls.copy()

print("Decade counts (post-filter):")
print(df_f["decade"].value_counts().sort_index())

if df_f["decade"].nunique() < 2:
    raise ValueError("Need at least 2 decades to train a multi-class classifier.")

# -------------------- 4) Train/test split (stratified when possible) --------------------
y_decade = df_f["decade"].astype(str)  # as strings for prettier reports

try:
    df_tr, df_te = train_test_split(
        df_f, test_size=0.2, stratify=y_decade, random_state=42
    )
except ValueError:
    # If stratified split fails (too few samples per class), fall back to non-stratified
    df_tr, df_te = train_test_split(df_f, test_size=0.2, random_state=42)

print({
    "total": len(df_f),
    "train": len(df_tr),
    "test": len(df_te),
    "n_classes_total": df_f["decade"].nunique(),
    "n_classes_train": df_tr["decade"].nunique(),
    "n_classes_test": df_te["decade"].nunique(),
})

# -------------------- 5) TF-IDF (fit on TRAIN → transform TEST) --------------------
tfidf = TfidfVectorizer(
    max_features=50_000,
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.9,
    stop_words="english"
)
X_text_tr = tfidf.fit_transform(df_tr["clean_abstract"])
X_text_te = tfidf.transform(df_te["clean_abstract"]) if len(df_te) else sp.csr_matrix((0, X_text_tr.shape[1]))

# -------------------- 6) Topic one-hot (last column is outlier −1) --------------------
def _one_hot_topics_fixed(labels_np: np.ndarray, K: int, n_non_out: int) -> sp.csr_matrix:
    labels_np = labels_np.astype(int)
    rows = np.arange(len(labels_np))
    cols = np.where(labels_np == -1, n_non_out, labels_np)
    data = np.ones(len(labels_np), dtype=np.float32)
    return sp.coo_matrix((data, (rows, cols)), shape=(len(labels_np), K)).tocsr()

topic_tr = df_tr["Topic"].astype("Int64").astype("float").fillna(-1).astype(int).to_numpy()
topic_te = df_te["Topic"].astype("Int64").astype("float").fillna(-1).astype(int).to_numpy() if len(df_te) else np.array([], dtype=int)

# Compute global topic dimension from train+test, with an outlier slot at the end
all_topics = np.concatenate([topic_tr, topic_te]) if len(topic_te) else topic_tr
has_non_out = (all_topics >= 0).any()
n_topics_global = int(all_topics[all_topics >= 0].max() + 1) if has_non_out else 0
K_topics = max(n_topics_global + 1, 1)  # ensure at least one column

X_topics_tr = _one_hot_topics_fixed(topic_tr, K_topics, n_topics_global)
X_topics_te = _one_hot_topics_fixed(topic_te, K_topics, n_topics_global) if len(topic_te) else sp.csr_matrix((0, K_topics))

# -------------------- 7) Combine features (NO year → no label leakage) --------------------
X_train = sp.hstack([X_text_tr, X_topics_tr], format="csr").astype(np.float32)
X_test  = sp.hstack([X_text_te,  X_topics_te ], format="csr").astype(np.float32)
print({"X_train": X_train.shape, "X_test": X_test.shape, "K_topics": K_topics})

# -------------------- 8) Labels & class weights --------------------
le = LabelEncoder()
le.fit(pd.concat([df_tr["decade"].astype(str), df_te["decade"].astype(str)], axis=0))
y_train = le.transform(df_tr["decade"].astype(str))
y_test  = le.transform(df_te["decade"].astype(str)) if len(df_te) else np.array([], dtype=int)

# Inverse-frequency class weights on TRAIN
train_counts = pd.Series(y_train).value_counts()
weights_per_class = train_counts.sum() / (len(train_counts) * train_counts)
sample_weights = np.array([float(weights_per_class[y]) for y in y_train])

# -------------------- 9) Train XGBoost (with safe early stopping) --------------------
dtrain = xgb.DMatrix(X_train, label=y_train, weight=sample_weights)

evals_list = [(dtrain, "train")]
use_valid = X_test.shape[0] > 0
if use_valid:
    dvalid = xgb.DMatrix(X_test, label=y_test)
    evals_list.append((dvalid, "valid"))

params = {
    "objective": "multi:softprob",
    "num_class": len(le.classes_),
    "eval_metric": "mlogloss",
    "max_depth": 6,
    "eta": 0.05,
    "min_child_weight": 1.0,
    "subsample": 0.9,
    "colsample_bytree": 0.8,
    "reg_lambda": 1.0,
    "reg_alpha": 0.1,
    "tree_method": "hist",
    "max_bin": 128,
    "seed": 42,
    "nthread": -1,
}

bst = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=3000,
    evals=evals_list,
    early_stopping_rounds=100 if use_valid else None,
    verbose_eval=50
)

# -------------------- 10) Predict & report --------------------
if use_valid:
    # Version-safe prediction
    try:
        best_iter = getattr(bst, "best_iteration", None)
        if best_iter is not None:
            y_proba = bst.predict(dvalid, iteration_range=(0, best_iter + 1))
        else:
            best_ntree_limit = getattr(bst, "best_ntree_limit", 0)
            y_proba = bst.predict(dvalid, ntree_limit=best_ntree_limit or None)
    except TypeError:
        y_proba = bst.predict(dvalid)

    y_pred_idx = y_proba.argmax(axis=1)
    y_pred = le.inverse_transform(y_pred_idx)
    y_true = le.inverse_transform(y_test)

    print("\nBalanced XGBoost decade classification report (NO YEAR feature):")
    print(classification_report(y_true, y_pred, digits=3))

    cm = confusion_matrix(y_true, y_pred, labels=le.classes_)
    cm_df = pd.DataFrame(
        cm,
        index=[f"true_{c}" for c in le.classes_],
        columns=[f"pred_{c}" for c in le.classes_]
    )
    print("\nConfusion matrix:")
    print(cm_df)

    print("\nTrue label distribution (test):")
    print(pd.Series(y_true).value_counts(normalize=True).round(3))
    print("\nPredicted label distribution:")
    print(pd.Series(y_pred).value_counts(normalize=True).round(3))
else:
    print("\n(No validation set — dataset too small or single common class.)")

print(f"\nDone in {time.time()-t0:.1f}s | TF-IDF={X_text_tr.shape[1]} | Topics={K_topics}")

import re
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# --- 1) Pull feature importances from the Booster ---
# bst is the Booster returned by xgb.train(...)
importances = bst.get_score(importance_type="gain")  # keys like 'f0','f1',...
imp_df = (pd.DataFrame(
    {"feature": list(importances.keys()), "gain": [float(v) for v in importances.values()]}
).sort_values("gain", ascending=False))

# --- 2) Map f-index -> readable feature names (TF-IDF, then topics) ---
tfidf_names = tfidf.get_feature_names_out().tolist()
# topics: 0..n_topics_global-1 plus an outlier slot at the end
topic_names = [f"__topic_{i}" for i in range(n_topics_global)] + ["__topic_outlier(-1)"]
feature_names = tfidf_names + topic_names  # NO 'year' feature in this leakage-safe run

# extract numeric index from 'f###' and map
idx_series = imp_df["feature"].str.extract(r"(\d+)")[0].astype("Int64")  # nullable int
def idx_to_name(x):
    if pd.isna(x):
        return "N/A"
    i = int(x)
    return feature_names[i] if i < len(feature_names) else "N/A"

imp_df["feature_name"] = idx_series.map(idx_to_name)

# --- 3) Filter out short/numeric/filler tokens (keep topics too) ---
def is_valid_feature(name: str) -> bool:
    if not isinstance(name, str) or name == "N/A":
        return False
    if name.startswith("__topic_"):   # always keep topic indicators
        return True
    s = name.strip().lower()
    s_clean = re.sub(r"[^a-z\s]", "", s).strip()
    if len(s_clean) < 3:
        return False
    if s_clean.isdigit():
        return False
    stoplike = {"et", "al", "pm", "ref", "fig", "sec", "see", "eq", "vs", "aa"}
    return s_clean not in stoplike

imp_df = imp_df[imp_df["feature_name"].apply(is_valid_feature)].copy()

# --- 4) Show & plot top-20 cleaned features ---
topk = imp_df.head(20)
print(topk[["feature_name", "gain"]])

plt.figure(figsize=(8, 6))
plt.barh(topk["feature_name"][::-1], topk["gain"][::-1])
plt.xlabel("Gain")
plt.title("Top 20 Feature Importances (Gain, cleaned)")
plt.tight_layout()
plt.show()

"""### **XGBoost Feature Importance — Full vs. Physics-Term Subset**

The two plots display the top-ranked features by **Gain** from the XGBoost model:  
the first includes all cleaned tokens (≥3 characters), while the second filters to **physics-specific terms** only.

**General feature importance (top plot):**  
- The model is dominated by a few high-weight textual artefacts such as **“ital ital”** and **“ital sup”**, which likely originate from formatting tags (e.g., LaTeX or markup remnants).  
- Beyond these, genuine scientific expressions such as **“direct experimental”**, **“diagrammatic loop”**, **“dilepton decay”**, and **“inflaton field”** emerge as strong contributors, reflecting collider phenomenology, field-theoretic formulations, and decay-channel analyses.  
- The presence of terms like **“baryogenesis”**, **“lepton decay”**, and **“energy collider”** further underscores the model’s sensitivity to key high-energy and cosmological physics themes.

**Physics-only feature importance (bottom plot):**  
- Here, meaningful domain terms dominate: **“bosons pm”**, **“antiproton collider”**, and **“diagrammatic loop”** carry the highest predictive gain, emphasizing collider- and field-theory-related content.  
- **“Supersymmetric extension”**, **“neutral Higgs”**, and **“heavy Higgs”** reflect Beyond Standard Model (BSM) and Higgs-sector relevance, aligning with key physics subfields in the corpus.  
- Mentions of **“dilepton decay”**, **“baryon decays”**, and **“energy colliders”** highlight experimental and detector-side observables that help distinguish topic clusters.

**Overall insight:**  
The XGBoost model effectively captures **domain-specific semantic cues** associated with collider and Higgs physics.  
While a few text-preprocessing artefacts inflate gain in the full set, the **physics-filtered view** reveals meaningful physical entities (bosons, Higgs, colliders, supersymmetry) as the main discriminative features — validating that the model’s learned structure aligns well with core concepts in high-energy phenomenology.

## Hierarchical Topic Structure
"""

import bertopic
print(bertopic.__version__)

from bertopic import BERTopic
import inspect

# Step 1: Prepare docs
docs_used = df_src["clean_abstract"].astype(str).tolist()

# Step 2: Compute hierarchical topics
# This returns a pandas DataFrame with columns like Parent_Name, Child_Name, etc.
hierarchical_topics_df = best_model.hierarchical_topics(docs_used)

# Step 3: Inspect visualize_hierarchy signature
sig = inspect.signature(best_model.visualize_hierarchy)
params = list(sig.parameters.keys())

# Step 4: Use correct call form based on signature
if "hierarchical_topics" in params:
    # Newer signature
    fig = best_model.visualize_hierarchy(
        hierarchical_topics=hierarchical_topics_df,
        orientation="left",
        top_n_topics=None,
        width=1100,
        height=800
    )
else:
    # Fallback for very old signatures (rare)
    fig = best_model.visualize_hierarchy(
        docs_used,
        hierarchical_topics_df,
        "left",
        None,
        1100,
        800
    )

# Step 5: Display the visualization
fig.show()

"""### **Comment: Hierarchical Topic Clustering**

The dendrogram shows two primary clusters:  
- **(0, 3, 5)** — QCD- and computation-oriented topics, reflecting shared methodological foundations.  
- **(1, 2, 4)** — Higgs and collider-focused themes, linked by experimental and phenomenological overlap.  

This structure highlights a clear division between **theoretical QCD/computational** and **experimental collider/Higgs** research streams within the corpus.

## Topic-drift & Temporal dynamics
"""

# 2) Yearly proportions + OLS slope (emerging/declining/stable)
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Build topic-year count matrix
df_tmp = df_src[["year", "Topic_final"]].dropna()
df_tmp["year"] = df_tmp["year"].astype(int)
df_tmp = df_tmp[df_tmp["Topic_final"] != -1]

topic_year_counts = (
    df_tmp.groupby(["year", "Topic_final"])
          .size()
          .unstack(fill_value=0)
          .sort_index()
)

# Row-normalized shares
topic_year_share = topic_year_counts.div(topic_year_counts.sum(axis=1), axis=0).fillna(0)

# OLS slope per topic
slopes = {}
for t in topic_year_share.columns:
    y = topic_year_share[t].values
    x = np.arange(len(y))
    X = sm.add_constant(x)
    model = sm.OLS(y, X).fit()
    slopes[t] = model.params[1]

slope_df = pd.Series(slopes, name="ols_slope").sort_values(ascending=False)

# Tag topics
thr = slope_df.abs().median()  # simple threshold
tags = slope_df.apply(lambda s: "emerging" if s > thr else ("declining" if s < -thr else "stable"))
tags.name = "trend_tag"

display(pd.concat([slope_df, tags], axis=1).head(10))

# Quick plot: smoothed 3-year rolling shares
plt.figure(figsize=(10,5))
topic_year_share.rolling(3, min_periods=1).mean().plot(ax=plt.gca(), lw=2)
plt.title("Smoothed Topic Trends (3-year rolling mean)")
plt.ylabel("Share of Publications")
plt.xlabel("Year")
plt.tight_layout()
plt.show()

"""### **Comment: Smoothed Topic Trends (3-Year Rolling Mean)**  
The plot illustrates long-term topic evolution, with **Topic 0** dominating post-1980 and others emerging gradually after 1990. Older pre-LHC activity (pre-1980) remains negligible, while post-2000 trends show **diversification and convergence** of research themes, indicating a mature, balanced field.

## Topic vs Year Heatmap; Top Keywords per Topic
"""

# Heatmap of topic share by year
import seaborn as sns
plt.figure(figsize=(11,6))
sns.heatmap(topic_year_share.T, cmap="mako", cbar_kws={"label":"Share"})
plt.title("Topic Share by Year")
plt.xlabel("Year"); plt.ylabel("Topic")
plt.tight_layout(); plt.show()

"""### **Comment: Topic Share by Year**

#### The heatmap shows the relative publication share of each topic over time.  
**Topic 0** dominates early and remains consistently active, while other topics (**1–5**) gain presence gradually after ~1995, reflecting the **expansion and diversification** of collider-era research.  
Recent years (post-2010) display a **balanced distribution**, indicating a mature and stable topic landscape.  
"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_top_keywords_grid(model: BERTopic, topic_ids, topn: int = 10):
    n_topics = len(topic_ids)
    ncols = 3
    nrows = (n_topics + ncols - 1) // ncols  # ceil division

    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 4 * nrows))
    axes = axes.flatten()

    for i, tid in enumerate(topic_ids):
        words = model.get_topic(tid)
        ax = axes[i]
        if words is None:
            ax.text(0.5, 0.5, f"No terms for topic {tid}", ha="center", va="center")
            ax.axis("off")
            continue

        terms, weights = zip(*words[:topn])
        sns.barplot(
            x=list(weights),
            y=[t.replace("_", " ") for t in terms],
            color="#1f77b4",
            ax=ax
        )
        ax.set_title(f"Top {topn} keywords — Topic {tid}: {topic_labels.get(tid, tid)}", fontsize=10)
        ax.set_xlabel("Weight")
        ax.set_ylabel("")

    # Hide any unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis("off")

    plt.tight_layout()
    plt.show()

# Example usage
top_topics = topic_year_counts.sum().sort_values(ascending=False).head(6).index.tolist()
plot_top_keywords_grid(best_model, top_topics, topn=12)

"""### **Comment: Top 12 Keywords per Topic**

The keyword distributions clearly distinguish the thematic focus of each topic:

- **Topic 0 – QCD Higgs LHC Quark:**  
  Core theoretical collider theme, dominated by *QCD*, *Higgs*, and *gluon* terms.  

- **Topic 1 – Higgs LHC Quark Collider:**  
  Experiment-oriented, featuring *detector*, *proton*, and *luminosity* — tied to collider data analysis.  

- **Topic 2 – QCD Quark Feynman Amplitude:**  
  Leans toward *model-building* and *supersymmetric (MSSM)* studies, mixing theory and phenomenology.  

- **Topic 3 – Higgs LHC Collider Quark:**  
  Centers on *renormalization*, *computation*, and *gauge* — highlighting numerical QCD work.  

- **Topic 4 – Higgs LHC MML Quark:**  
  Strong *experimental–institutional* signature (*CERN*, *luminosity*, *neutrino*), reflecting detector-level analyses.  

- **Topic 5 – QCD LHC Quark Collider:**  
  Focuses on *computation*, *loop*, and *Feynman* terms — characteristic of **perturbative and numerical methods**.

**Overall:**  
The topics split cleanly between **theoretical (QCD, computation)** and **experimental (LHC, detector, luminosity)** domains, confirming a balanced representation of collider physics research.

## Summary, Limitations, and Next Steps

### Key Insights
1. **Collider/Higgs dominance**: Topic 0 (QCD–LHC–Higgs–Quark) remains consistently dominant across decades, especially post-2000.  
2. **Stabilization phase**: Forecast growth scores are negative across topics, indicating **steady or mildly declining** near-term output—consistent with a mature, post-commissioning LHC era.  
3. **Predictive sanity**: ARIMA backtests achieve **MAPE ≈ 0.17** on short horizons, supporting the reliability of near-term trend forecasts.

### Limitations
- **Model simplicity**: I intentionally use default BERTopic and ARIMA(1,1,1) settings for interpretability; richer models could capture nonlinearity.  
- **Corpus coverage**: The citation-expanded corpus focuses on arXiv/INSPIRE pipelines; domain biases or missing metadata may affect topic prevalence.

### Next Steps
- **ML-HEP corpus**: Build a dedicated **machine-learning in HEP** corpus via arXiv/INSPIRE queries and model it separately for comparative trends.  
- **Validation add-ons**: Report topic **diversity/coherence** metrics and add a brief **ARIMA rolling backtest table** across multiple horizons (1–5 years).
"""